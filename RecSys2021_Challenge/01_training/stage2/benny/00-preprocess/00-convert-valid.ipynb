{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a384a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA CORPORATION\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "other-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-audit",
   "metadata": {},
   "source": [
    "# Rename and Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "seventh-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/raid/recsys2021_valid/*?*')\n",
    "for file in files:\n",
    "    os.system('mv \"' + file + '\" ' + file.split(\"?X\")[0])\n",
    "files = glob.glob('/raid/recsys2021_valid/*?*')\n",
    "for file in files:\n",
    "    os.system('mv \"' + file + '\" ' + file.split(\"?\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "saving-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(glob.glob('/raid/recsys2021_valid/*'))\n",
    "files = [x for x in files if '.lzo' in x]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "associate-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    os.system('rm -r ' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "whole-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    }
   ],
   "source": [
    "!rm -r /results/recsys2021_valid/\n",
    "!mkdir /results/recsys2021_valid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "immediate-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    print('cp ' + file + ' ' + file.replace('/raid/', '/results/'))\n",
    "    os.system('cp ' + file + ' ' + file.replace('/raid/', '/results/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-attraction",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numerous-cedar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 21.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: transformers in /opt/conda/envs/rapids/lib/python3.8/site-packages (4.6.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 31.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/envs/rapids/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/rapids/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /opt/conda/envs/rapids/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/rapids/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/rapids/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: sentencepiece, emoji\n",
      "Successfully installed emoji-1.2.0 sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "emerging-printer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /opt/conda/envs/rapids/lib/python3.8/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/rapids/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/rapids/lib/python3.8/site-packages (from torch) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extended-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import datetime\n",
    "import hashlib\n",
    "import emoji\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "undefined-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    'text_tokens',    ###############\n",
    "    'hashtags',       #Tweet Features\n",
    "    'tweet_id',       #\n",
    "    'media',          #\n",
    "    'links',          #\n",
    "    'domains',        #\n",
    "    'tweet_type',     #\n",
    "    'language',       #\n",
    "    'timestamp',      ###############\n",
    "    'a_user_id',              ###########################\n",
    "    'a_follower_count',       #Engaged With User Features\n",
    "    'a_following_count',      #\n",
    "    'a_is_verified',          #\n",
    "    'a_account_creation',     ###########################\n",
    "    'b_user_id',              #######################\n",
    "    'b_follower_count',       #Engaging User Features\n",
    "    'b_following_count',      #\n",
    "    'b_is_verified',          #\n",
    "    'b_account_creation',     #######################\n",
    "    'b_follows_a',    #################### Engagement Features\n",
    "    'reply',          #Target Reply\n",
    "    'retweet',        #Target Retweet    \n",
    "    'retweet_comment',#Target Retweet with comment\n",
    "    'like',           #Target Like\n",
    "                      ####################\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respiratory-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_MEDIA = {\n",
    " '': 0,\n",
    " 'Photo': 1,\n",
    " 'Photo\\tPho': 2,\n",
    " 'Video': 3,\n",
    " 'GIF': 4,\n",
    " 'Video\\tVid': 5,\n",
    " 'Photo\\tVid': 6,\n",
    " 'Video\\tPho': 7,\n",
    " 'GIF\\tPhoto': 8,\n",
    " 'Photo\\tGIF': 9,\n",
    " 'GIF\\tGIF': 10,\n",
    " 'GIF\\tVideo': 11,\n",
    " 'Video\\tGIF': 12,\n",
    " 'GIF\\tGIF\\tG': 13\n",
    "}\n",
    "\n",
    "MAP_TYPE = {'TopLevel': 0, 'Retweet': 1, 'Quote': 2}\n",
    "\n",
    "MAP_LANG = {\n",
    " '488B32D24BD4BB44172EB981C1BCA6FA': 0,\n",
    " 'E7F038DE3EAD397AEC9193686C911677': 1,\n",
    " 'B0FA488F2911701DD8EC5B1EA5E322D8': 2,\n",
    " 'B8B04128918BBF54E2E178BFF1ABA833': 3,\n",
    " '313ECD3A1E5BB07406E4249475C2D6D6': 4,\n",
    " '1F73BB863A39DB62B4A55B7E558DB1E8': 5,\n",
    " '9FCF19233EAD65EA6E32C2E6DC03A444': 6,\n",
    " '9A78FC330083E72BE0DD1EA92656F3B5': 7,\n",
    " '8729EBF694C3DAF61208A209C2A542C8': 8,\n",
    " 'E6936751CBF4F921F7DE1AEF33A16ED0': 9,\n",
    " '7F4FAB1EB12CD95EDCD9DB2A6634EFCE': 10,\n",
    " 'B4DC2F82961F1263E90DF7A942CCE0B2': 11,\n",
    " '310ECD7D1E42216E3C1B31EFDDFC72A7': 12,\n",
    " '5A0759FB938B1D9B1E08B7A3A14F1042': 13,\n",
    " '2F548E5BE0D7F678E72DDE31DFBEF8E7': 14,\n",
    " '5B6973BEB05212E396F3F2DC6A31B71C': 15,\n",
    " '2573A3CF633EBE6932A1E1010D5CD213': 16,\n",
    " 'DA13A5C3763C212D9D68FC69102DE5E5': 17,\n",
    " '00304D7356D6C64481190D708D8F739C': 18,\n",
    " '7D11A7AA105DAB4D6799AF863369DB9C': 19,\n",
    " '23686A079CA538645BF6118A1EF51C8B': 20,\n",
    " 'A5CFB818D79497B482B7225887DBD3AD': 21,\n",
    " '838A92D9F7EB57FB4A8B0C953A80C7EB': 22,\n",
    " '99CA116BF6AA65D70F3C78BEBADC51F0': 23,\n",
    " 'D922D8FEA3EFAD3200455120B75BCEB8': 24,\n",
    " '159541FA269CA8A9CDB93658CAEC4CA2': 25,\n",
    " 'E84BE2C963852FB065EE827F41A0A304': 26,\n",
    " '6B90065EA806B8523C0A6E56D7A961B2': 27,\n",
    " '4B55C45CD308068E4D0913DEF1043AD6': 28,\n",
    " 'BAC6A3C2E18C26A77C99B41ECE1C738D': 29,\n",
    " '4CA37504EF8BA4352B03DCBA50E98A45': 30,\n",
    " '3228B1FB4BC92E81EF2FE35BDA86C540': 31,\n",
    " 'D7C16BC3C9A5A633D6A3043A567C95A6': 32,\n",
    " '477ED2ED930405BF1DBF13F9BF973434': 33,\n",
    " '41776FB50B812A6775C2F8DEC92A9779': 34,\n",
    " 'C1E99BF67DDA2227007DE8038FE32470': 35,\n",
    " 'F70598172AC4514B1E6818EA361AD580': 36,\n",
    " '6744F8519308FD72D8C47BD45186303C': 37,\n",
    " '10C6C994C2AD434F9D49D4BE9CFBC613': 38,\n",
    " '89CE0912454AFE0A1B959569C37A5B8F': 39,\n",
    " '105008E45831ADE8AF1DB888319F422A': 40,\n",
    " 'DE8A3755FCEDC549A408D7B1EB1A2C9F': 41,\n",
    " 'BF04E736C599E9DE22F39F1DC157E1F1': 42,\n",
    " 'CF304ED3CFC1ADD26720B97B39900FFD': 43,\n",
    " '59BE899EB83AAA19878738040F6828F0': 44,\n",
    " '3DF931B225B690508A63FD24133FA0E2': 45,\n",
    " '3AB05D6A4045A6C37D3E4566CFDFFE26': 46,\n",
    " '678E280656F6A0C0C23D5DFD46B85C14': 47,\n",
    " '440116720BC3A7957E216A77EE5C18CF': 48,\n",
    " 'A3E4360031A7E05E9279F4D504EE18DD': 49,\n",
    " 'C41F6D723AB5D14716D856DF9C000DED': 50,\n",
    " '7E18F69967284BB0601E88A114B8F7A9': 51,\n",
    " 'F9D8F1DB5A398E1225A2C42E34A51DF6': 52,\n",
    " '914074E75CB398B5A2D81E1A51818CAA': 53,\n",
    " '5B210378BE9FFA3C90818C43B29B466B': 54,\n",
    " 'F33767F7D7080003F403FDAB34FEB755': 55,\n",
    " 'DC5C9FB3F0B3B740BAEE4F6049C2C7F1': 56,\n",
    " '3EA57373381A56822CBBC736169D0145': 57,\n",
    " '37342508F52BF4B62CCE3BA25460F9EB': 58,\n",
    " '7168CE9B777B76E4069A538DC5F28B6F': 59,\n",
    " '0BB2C843174730BA7D958C98B763A797': 60,\n",
    " 'CDE47D81F953D800F760F1DE8AA754BA': 61,\n",
    " '9D831A0F3603A54732CCBDBF291D17B7': 62,\n",
    " '5F152815982885A996841493F2757D91': 63,\n",
    " '82C9890E4A7FC1F8730A3443C761143E': 64,\n",
    " '8C64085F46CD49FA5C80E72A35845185': 65}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashit(x):\n",
    "    uhash = '0' if len(x)<=2 else x\n",
    "    hash_object = hashlib.md5(uhash.encode('utf-8'))\n",
    "    return int(hash_object.hexdigest(),16)%2**32\n",
    "\n",
    "\n",
    "def extract_hash(text, split_text='@', no=0):\n",
    "    text = text.lower()\n",
    "    uhash = ''\n",
    "    text_split = text.split('@')\n",
    "    if len(text_split)>(no+1):\n",
    "        text_split = text_split[no+1].split(' ')\n",
    "        cl_loop = True\n",
    "        uhash += clean_text(text_split[0])\n",
    "        while cl_loop:\n",
    "            if len(text_split)>1:\n",
    "                if text_split[1] in ['_']:\n",
    "                    uhash += clean_text(text_split[1]) + clean_text(text_split[2])\n",
    "                    text_split = text_split[2:]\n",
    "                else:\n",
    "                    cl_loop = False\n",
    "            else:\n",
    "                cl_loop = False\n",
    "                \n",
    "    return hashit(uhash)\n",
    "\n",
    "def clean_text(text):\n",
    "    if len(text)>1:\n",
    "        if text[-1] in ['!', '?', ':', ';', '.', ',']:\n",
    "            return(text[:-1])\n",
    "    return(text)\n",
    "\n",
    "def ret_word( x, rw=0 ):\n",
    "    x = x.split(' ')\n",
    "    \n",
    "    if len(x)>rw:\n",
    "        return hashit(x[rw])\n",
    "    elif rw<0:\n",
    "        if len(x)>0:\n",
    "            return hashit(x[-1])\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def extract_rt(x_org):\n",
    "    x = x_org.replace('[sep]', '')\n",
    "    x = x.split('http')[0]\n",
    "    x = x.rstrip()\n",
    "    return(x)\n",
    "\n",
    "def check_last_char_quest(x_org):\n",
    "    if len(x_org)<1:\n",
    "        return(0)\n",
    "    x = x_org.replace('[sep]', '')\n",
    "    x = x.split('http')[0]\n",
    "    if '#' in x:\n",
    "        x = x.split('#')[0] + ' '.join(x.split('#')[1].split(' ')[1:])\n",
    "    if '@' in x:\n",
    "        x = x.split('@')[0] + ' '.join(x.split('@')[1].split(' ')[1:])\n",
    "    x = x.rstrip()\n",
    "    if len(x)<2:\n",
    "        return(0)\n",
    "    elif x[-1]=='?' and x[-2]!='!':\n",
    "        return(1)\n",
    "    elif x[-1]=='?' and x[-2]=='!':\n",
    "        return(2)\n",
    "    elif x[-1]=='!' and x[-2]=='?':\n",
    "        return(3)\n",
    "    elif x[-1]=='!' and x[-2]!='?':\n",
    "        return(4)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rising-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    }
   ],
   "source": [
    "!rm -r /raid/recsys2021_valid_pre\n",
    "!mkdir /raid/recsys2021_valid_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considered-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/raid/recsys2021_valid/*')\n",
    "files = [file for file in files if '.lzo' not in file and 'validation_urls.txt' not in file]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bridal-gabriel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/recsys2021_valid/part-00000_1', '/raid/recsys2021_valid/part-00000_2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forced-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def extract_feature(fn):\n",
    "\n",
    "    df = pd.read_csv(fn, sep='\\x01', header=None)\n",
    "    df.columns = all_features\n",
    "\n",
    "    filenumber = int(fn.split('/')[-1].split('-')[-1])\n",
    "\n",
    "    #Only run in trainset and not in test\n",
    "    if 'like' in df.columns: # do this file contains the target?\n",
    "        df['reply'] = df['reply'].fillna(0)\n",
    "        df['retweet'] = df['retweet'].fillna(0)\n",
    "        df['retweet_comment'] = df['retweet_comment'].fillna(0)\n",
    "        df['like'] = df['like'].fillna(0)    \n",
    "        df.loc[df.reply>0,'reply'] = 1\n",
    "        df.loc[df.retweet>0,'retweet'] = 1\n",
    "        df.loc[df.retweet_comment>0,'retweet_comment'] = 1\n",
    "        df.loc[df.like>0,'like'] = 1\n",
    "        df['reply'] = df['reply'].astype(np.int8)\n",
    "        df['retweet'] = df['retweet'].astype(np.int8)\n",
    "        df['retweet_comment'] = df['retweet_comment'].astype(np.int8)\n",
    "        df['like'] = df['like'].astype(np.int8)\n",
    "    \n",
    "    ###########################\n",
    "    #Tweet token processing####\n",
    "    ###########################\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    val = df['text_tokens'].values.copy()\n",
    "    for n,v in enumerate(val):\n",
    "        val[n] = tokenizer.decode(v.split('\\t'))    \n",
    "    df['text'] = val\n",
    "\n",
    "    ##########################################################################################    \n",
    "    df['tw_len_media'] = df['media'].apply(lambda x: str(x).count('\\t')+1 if not(pd.isnull(x)) else 0).astype(np.int8)\n",
    "    df['tw_len_photo'] = df['media'].apply(lambda x: str(x).count('Photo') if not(pd.isnull(x)) else 0).astype(np.int8)\n",
    "    df['tw_len_video'] = df['media'].apply(lambda x: str(x).count('Video') if not(pd.isnull(x)) else 0).astype(np.int8)\n",
    "    df['tw_len_gif'] = df['media'].apply(lambda x: str(x).count('GIF') if not(pd.isnull(x)) else 0).astype(np.int8)\n",
    "    df['tw_len_quest'] = df['text'].apply(lambda x: str(x).count('?')).astype(np.int8)\n",
    "    df['tw_len_token'] = df['text_tokens'].apply(lambda x: str(x).count('\\t')).astype(np.int16)\n",
    "    df['tw_count_capital_words'] = df['text'].apply(lambda x: len(re.findall(r'\\b[A-Z]{2,}\\b', x)) ).astype(np.int16)\n",
    "    df['tw_count_excl_quest_marks'] = df['text'].apply(lambda x: len(re.findall(r'!|\\?', x)) ).astype(np.int16)\n",
    "    df['tw_count_special1'] = df['text'].str.count('¶').astype(np.int16)\n",
    "    df['tw_count_hash'] = df['text'].str.count('#').astype(np.int16)\n",
    "    df['tw_last_quest'] = df['text'].apply(lambda x: check_last_char_quest(x) ).astype(np.int8)\n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x: x.lower() )\n",
    "    df['text'] = df['text'].apply( lambda x: x.replace('http : / / t. co / ', 'http') )\n",
    "    df['text'] = df['text'].apply( lambda x: x.replace('https : / / t. co / ', 'http') )\n",
    "    df['text'] = df['text'].apply(lambda x: x[0:-5] )\n",
    "    df['text'] = df['text'].apply( lambda x: x.replace(' _ ', '_') )\n",
    "    df['text'] = df['text'].apply( lambda x: x.replace('@ ', '@') )\n",
    "    df['text'] = df['text'].apply( lambda x: x.replace('# ', '#') )\n",
    "    \n",
    "    df['tw_len_retweet'] = df['text'].apply(lambda x: str(x).count('retweet')).astype(np.int8)    \n",
    "    df['tw_isrt'] = (df['tweet_type']=='Retweet').astype(np.int8)\n",
    "    df['text'] = df['text'].str.replace('\\[cls\\] rt @', '@')\n",
    "    df['text'] = df['text'].str.replace('\\[cls\\] ', ' ')\n",
    "    df['tw_len_rt'] = df['text'].apply(lambda x: str(extract_rt(x)).count(' rt ')).astype(np.int8)\n",
    "    \n",
    "    # Split retweet text and original text\n",
    "    df['rt_text'] = df.apply(lambda x: '' if x['tw_isrt']==0 else x['text'].split(':')[0], axis=1)\n",
    "    df['text'] = df.apply(lambda x: x['text'] if x['tw_isrt']==0 else ':'.join(x['text'].split(':')[1:]) , axis=1)\n",
    "    \n",
    "    df['tw_count_at'] = df['text'].str.count('@').astype(np.int16)\n",
    "    df['text'] = df['text'].apply( lambda x: x.replace('¶ ', ' ') )\n",
    "    \n",
    "    df['rt_text'] = df['rt_text'].apply( lambda x: x.replace('¶ ', ' ') )\n",
    "    df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "    \n",
    "    df['rt_text'] = df['rt_text'].apply(lambda x: x.strip())\n",
    "    df['text'] = df['text'].apply( lambda x: \" \".join(x.split()) )\n",
    "    \n",
    "    df['rt_text'] = df['rt_text'].apply( lambda x: \" \".join(x.split()) )\n",
    "    \n",
    "    df['tw_count_words'] = df['text'].str.count(' ').astype(np.int16)\n",
    "    df['tw_count_char']  = df['text'].apply(lambda x: len(x)).astype(np.int16)\n",
    "    df['tw_rt_count_words'] = df['rt_text'].str.count(' ').astype(np.int16)\n",
    "    df['tw_rt_count_char']  = df['rt_text'].apply(lambda x: len(x)).astype(np.int16)\n",
    "    df['tw_original_user0'] = df['text'].apply(lambda x: extract_hash(x, no=0)   )\n",
    "    df['tw_original_user1'] = df['text'].apply(lambda x: extract_hash(x, no=1)   )\n",
    "    df['tw_original_user2'] = df['text'].apply(lambda x: extract_hash(x, no=2)   )\n",
    "    df['tw_rt_user0'] = df['rt_text'].apply(lambda x: extract_hash(x, no=0)   )\n",
    "    df['tw_original_http0'] = df['text'].apply(lambda x: extract_hash(x, split_text='http', no=0)   )\n",
    "    \n",
    "    df['tw_word0'] = df['text'].apply(lambda x: ret_word(x,0)).astype(np.int32)\n",
    "    df['tw_word1'] = df['text'].apply(lambda x: ret_word(x,1)).astype(np.int32)\n",
    "    df['tw_word2'] = df['text'].apply(lambda x: ret_word(x,2)).astype(np.int32)\n",
    "    df['tw_word3'] = df['text'].apply(lambda x: ret_word(x,3)).astype(np.int32)\n",
    "    df['tw_word4'] = df['text'].apply(lambda x: ret_word(x,4)).astype(np.int32)\n",
    "    df['tw_tweet'] = df['text'].apply(lambda x: hashit(x) ).astype(np.int32)\n",
    "    ##########################################################################################    \n",
    "    \n",
    "    \n",
    "    ##########################################################################################    \n",
    "    df['group'] = 0\n",
    "    df['group'] = df['group'] + 1*(df['a_follower_count']>=222)\n",
    "    df['group'] = df['group'] + 1*(df['a_follower_count']>=578)\n",
    "    df['group'] = df['group'] + 1*(df['a_follower_count']>=1225)\n",
    "    df['group'] = df['group'] + 1*(df['a_follower_count']>=3689)\n",
    "    df['group'] = df['group'].astype(np.int8)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df['dt_day']  = df['date'].dt.day.astype(np.int8)\n",
    "    df['dt_dow']  = df['date'].dt.weekday.astype(np.int8)\n",
    "    df['dt_minute'] = df['date'].dt.hour.astype(np.int16) * 60 + df['date'].dt.minute.astype(np.int16)\n",
    "    del df['date']\n",
    "    \n",
    "    df['len_hashtags'] = df['hashtags'].apply( lambda x:  len(x.split('\\t')) if x==x else 0 ).astype(np.int16)\n",
    "    df['len_links'] = df['links'].apply( lambda x: len(x.split('\\t')) if x==x else 0 ).astype(np.int16)\n",
    "    df['len_domains'] = df['domains'].apply( lambda x: len(x.split('\\t')) if x==x else 0 ).astype(np.int16)\n",
    "    \n",
    "    df['hashtags'] = df['hashtags'].apply( lambda x:  int(x.split('\\t')[0],16)%2**32 if x==x else 0 ).astype(np.int32)\n",
    "    df['links'] = df['links'].apply( lambda x: int(x.split('\\t')[0],16)%2**32 if x==x else 0 ).astype(np.int32)\n",
    "    df['domains'] = df['domains'].apply( lambda x: int(x.split('\\t')[0],16)%2**32 if x==x else 0 ).astype(np.int32)\n",
    "        \n",
    "    df['media'] = df['media'].apply(lambda x: MAP_MEDIA[x[:9]] if x==x else 0).astype(np.int8)\n",
    "    df['tweet_type'] = df['tweet_type'].apply(lambda x: MAP_TYPE[x] if x==x else 0).astype(np.int8)\n",
    "    df['language'] = df['language'].apply(lambda x: MAP_LANG[x] if x==x else 0).astype(np.int8)\n",
    "    \n",
    "    df['timestamp'] = df['timestamp'].astype(np.uint32)\n",
    "    \n",
    "    df.loc[ df.a_account_creation<0 ,'a_account_creation'] = 1138308613\n",
    "    df['a_account_creation'] = 240*(df['a_account_creation'] - 1138308613)/(1139000000 - 1138308613) - 127\n",
    "    df['a_account_creation'] = df['a_account_creation'].astype(np.int8)\n",
    "    \n",
    "    df.loc[ df.b_account_creation<0 ,'b_account_creation'] = 1138308613\n",
    "    df['b_account_creation'] = 240*(df['b_account_creation'] - 1138308613)/(1139000000 - 1138308613) - 127\n",
    "    df['b_account_creation'] = df['b_account_creation'].astype(np.int8)\n",
    "\n",
    "    df['a_follower_count'] = df['a_follower_count'].astype(np.int32)\n",
    "    df['a_following_count'] = df['a_following_count'].astype(np.int32)\n",
    "    df['b_follower_count'] = df['b_follower_count'].astype(np.int32)\n",
    "    df['b_following_count'] = df['b_following_count'].astype(np.int32)\n",
    "\n",
    "    df['a_is_verified'] = df['a_is_verified'].astype(np.int8)\n",
    "    df['b_is_verified'] = df['b_is_verified'].astype(np.int8)\n",
    "    df['b_follows_a'] = df['b_follows_a'].astype(np.int8)\n",
    "    \n",
    "    df['tweet_id'] = df['tweet_id'].apply(lambda x: int(x[-16:],16) ).astype(np.int64)\n",
    "    df['a_user_id'] = df['a_user_id'].apply(lambda x: int(x[-16:],16) ).astype(np.int64)\n",
    "    df['b_user_id'] = df['b_user_id'].apply(lambda x: int(x[-16:],16) ).astype(np.int64)\n",
    "    ##########################################################################################    \n",
    "    \n",
    "    del df['text_tokens']#Comment if you want to write strings to disk\n",
    "    del df['rt_text']    #Comment if you want to write strings to disk\n",
    "    del df['tw_isrt']\n",
    "    del df['text']\n",
    "        \n",
    "    df.to_parquet( '/raid/recsys2021_valid_pre/' + fn.split('/')[-1] + '.parquet'  )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stone-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58min 9s, sys: 2min 31s, total: 1h 40s\n",
      "Wall time: 1h 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "extract_feature(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "organized-steam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir /raid/recsys2021_valid_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "changing-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(fn):\n",
    "\n",
    "    df = pd.read_csv(fn, sep='\\x01', header=None)\n",
    "    df.columns = all_features\n",
    "\n",
    "    filenumber = int(fn.split('/')[-1].split('-')[-1])\n",
    "\n",
    "    #Only run in trainset and not in test\n",
    "    if 'like' in df.columns: # do this file contains the target?\n",
    "        df['reply'] = df['reply'].fillna(0)\n",
    "        df['retweet'] = df['retweet'].fillna(0)\n",
    "        df['retweet_comment'] = df['retweet_comment'].fillna(0)\n",
    "        df['like'] = df['like'].fillna(0)    \n",
    "        df.loc[df.reply>0,'reply'] = 1\n",
    "        df.loc[df.retweet>0,'retweet'] = 1\n",
    "        df.loc[df.retweet_comment>0,'retweet_comment'] = 1\n",
    "        df.loc[df.like>0,'like'] = 1\n",
    "        df['reply'] = df['reply'].astype(np.int8)\n",
    "        df['retweet'] = df['retweet'].astype(np.int8)\n",
    "        df['retweet_comment'] = df['retweet_comment'].astype(np.int8)\n",
    "        df['like'] = df['like'].astype(np.int8)\n",
    "    \n",
    "    df['media'] = df['media'].apply(lambda x: MAP_MEDIA[x[:9]] if x==x else 0).astype(np.int8)\n",
    "    df['tweet_type'] = df['tweet_type'].apply(lambda x: MAP_TYPE[x] if x==x else 0).astype(np.int8)\n",
    "    df['language'] = df['language'].apply(lambda x: MAP_LANG[x] if x==x else 0).astype(np.int8)\n",
    "    \n",
    "    df['timestamp'] = df['timestamp'].astype(np.uint32)\n",
    "    \n",
    "    df['tweet_id'] = df['tweet_id'].apply(lambda x: int(x[-16:],16) ).astype(np.int64)\n",
    "    df['a_user_id'] = df['a_user_id'].apply(lambda x: int(x[-16:],16) ).astype(np.int64)\n",
    "    df['b_user_id'] = df['b_user_id'].apply(lambda x: int(x[-16:],16) ).astype(np.int64)\n",
    "    ##########################################################################################    \n",
    "        \n",
    "    df[[\n",
    "        'text_tokens',\n",
    "        'tweet_id',\n",
    "        'a_user_id',\n",
    "        'b_user_id',\n",
    "        'timestamp',\n",
    "        'media',\n",
    "        'tweet_type',\n",
    "        'language',\n",
    "        'reply',          #Target Reply\n",
    "        'retweet',        #Target Retweet    \n",
    "        'retweet_comment',#Target Retweet with comment\n",
    "        'like'\n",
    "    ]].to_parquet( '/raid/recsys2021_valid_token/' + fn.split('/')[-1] + '.parquet'  )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sharp-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 34.1 s, total: 3min 35s\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "extract_feature(files[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
