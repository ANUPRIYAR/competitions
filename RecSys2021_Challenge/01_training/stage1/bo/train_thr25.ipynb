{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3244b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA CORPORATION\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08de3e8e-f1f3-4844-b8e2-33a0686956e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]='1'\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import cupy\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "from util import compute_rce_fast\n",
    "\n",
    "DP = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))>1\n",
    "DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ebd93d-2073-45ce-ba0a-e054620f4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1+cu101'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "from nvtabular.framework_utils.torch.models import Model\n",
    "from nvtabular.framework_utils.torch.utils import process_epoch\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142d0ca-20ef-4588-8f78-1c1cc0bed872",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5357a2-1e6d-4544-8229-a664bb92af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatedEmbeddings(torch.nn.Module):\n",
    "    \"\"\"Map multiple categorical variables to concatenated embeddings.\n",
    "    Args:\n",
    "        embedding_table_shapes: A dictionary mapping column names to\n",
    "            (cardinality, embedding_size) tuples.\n",
    "        dropout: A float.\n",
    "    Inputs:\n",
    "        x: An int64 Tensor with shape [batch_size, num_variables].\n",
    "    Outputs:\n",
    "        A Float Tensor with shape [batch_size, embedding_size_after_concat].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_table_shapes, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Embedding(cat_size, emb_size) #, sparse=(cat_size > 1e5))\n",
    "                for cat_size, emb_size in embedding_table_shapes.values()\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        # first two cat columns (a_user and b_user) share same emb table            \n",
    "        x = [self.embedding_layers[0](x[:,0])] + [layer(x[:, i+1]) for i, layer in enumerate(self.embedding_layers)] \n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dbc381f-e5c0-40c8-ba0f-a4081f042fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "class Swish(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class Swish_Module(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return Swish.apply(x)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "bert_type = 'distilbert-base-multilingual-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, layers, embedding_table_shapes, dropout=0.2, bert_type=None, gru_dim=128, emb_dim=768):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.initial_cat_layer = ConcatenatedEmbeddings(embedding_table_shapes, dropout=dropout)\n",
    "        embedding_size = sum(emb_size for _, emb_size in embedding_table_shapes.values())\n",
    "        layers = [layers] if type(layers) is int else layers\n",
    "        layers = [num_features + gru_dim + embedding_size + 128 + 128] + layers\n",
    "        self.use_bert = True\n",
    "        self.embed = AutoModel.from_pretrained(bert_type).embeddings.word_embeddings  \n",
    "        assert emb_dim == self.embed.embedding_dim\n",
    "#             self.reduce_dim = nn.Linear(self.embed.embedding_dim, 256)\n",
    "#             self.embed = nn.Embedding(119547, emb_dim)\n",
    "#         layers[0] += gru_dim\n",
    "        self.lstm = nn.GRU(emb_dim, gru_dim, batch_first=True, bidirectional=False)    \n",
    "#             self.lstm = nn.Linear(self.embed.embedding_dim, gru_dim)\n",
    "\n",
    "        self.fn_layers = nn.ModuleList(\n",
    "                            nn.Sequential(\n",
    "                                nn.Dropout(p=dropout),\n",
    "                                nn.Linear(layers[i], layers[i+1]),\n",
    "                                nn.BatchNorm1d(layers[i+1]),\n",
    "                                Swish_Module(),\n",
    "                            )  for i in range(len(layers) -1)\n",
    "                         )        \n",
    "        self.fn_last = nn.Linear(layers[-1],4)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont, bert_tok):\n",
    "        a_emb = self.initial_cat_layer.embedding_layers[0](x_cat[:,0])\n",
    "        b_emb = self.initial_cat_layer.embedding_layers[0](x_cat[:,1])\n",
    "        mf = a_emb * b_emb        \n",
    "        \n",
    "        x_cat = self.initial_cat_layer(x_cat)\n",
    "        bert_tok = self.embed(bert_tok)#.mean(dim=1)\n",
    "#             bert_tok = self.reduce_dim(bert_tok)\n",
    "        lstm_out = self.lstm(bert_tok)[0][:,-1]\n",
    "        output = torch.cat([x_cont, lstm_out, x_cat, mf],dim=1)\n",
    "        for layer in self.fn_layers:\n",
    "            output = layer(output)\n",
    "        logit = self.fn_last(output)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffdaf6f-5b69-4271-a6fd-6469f4209e8e",
   "metadata": {},
   "source": [
    "## scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae1318b-1d72-4949-b739-65aeb46886f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e6868-78c9-4c24-b9af-f21d163f1e00",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4fcf98-08b0-4857-a152-b452e9bc1e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for batch in bar:\n",
    "        x_cat, x_cont, text_tok, targets = batch\n",
    "        \n",
    "        x_cat = x_cat.cuda()\n",
    "        x_cont = x_cont.cuda()\n",
    "        text_tok = text_tok.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         optimizer2.zero_grad()\n",
    "\n",
    "        if use_torch_amp:\n",
    "            with amp.autocast():\n",
    "                logits = model(x_cat, x_cont, text_tok)\n",
    "#                 logits = model(data)\n",
    "            loss = criterion(logits, targets)       \n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # You can choose which optimizers receive explicit unscaling, if you\n",
    "            # want to inspect or modify the gradients of the params they own.\n",
    "            scaler.unscale_(optimizer)\n",
    "#             scaler.unscale_(optimizer2)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "#             scaler.step(optimizer2)\n",
    "\n",
    "            scaler.update()            \n",
    "            \n",
    "        elif use_amp:\n",
    "            logits = model(x_cat, x_cont, text_tok)\n",
    "#             logits = model(data)\n",
    "            loss = criterion(logits, targets)\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            logits = model(x_cat, x_cont, text_tok)\n",
    "#             logits = model(data)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_np = loss.item()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-50:]) / min(len(train_loss), 50)\n",
    "        bar.set_description('loss: %.4f, smth: %.4f' % (loss_np, smooth_loss))\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def valid_epoch(model, loader):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    LOGITS = []\n",
    "    TARGETS = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            x_cat, x_cont, text_tok, targets = batch\n",
    "\n",
    "            x_cat = x_cat.cuda()\n",
    "            x_cont = x_cont.cuda()\n",
    "            text_tok = text_tok.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "            logits = model(x_cat, x_cont, text_tok)\n",
    "#             logits = model(data)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss.append(loss.item())\n",
    "            LOGITS.append(logits.cpu())\n",
    "            TARGETS.append(targets.cpu())\n",
    "            \n",
    "    LOGITS = torch.cat(LOGITS)\n",
    "    TARGETS = torch.cat(TARGETS)\n",
    "    rce = {}\n",
    "    for i in range(4):\n",
    "        rce[label_names[i]] = compute_rce_fast(cp.asarray(LOGITS[:,i].sigmoid()),cp.asarray(TARGETS[:,i])).get()            \n",
    "    mean_rce = np.mean([v for k,v in rce.items()])\n",
    "            \n",
    "    val_loss = np.mean(val_loss)\n",
    "\n",
    "    return val_loss, rce, mean_rce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c66106-c3f9-47b2-b2c9-e618a51aab2e",
   "metadata": {},
   "source": [
    "# NVT loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e27c51-d7f4-4f6d-b892-2d0e8fecc0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = sorted(['reply', 'retweet', 'retweet_comment', 'like'])\n",
    "CAT_COLUMNS = ['a_user_id','b_user_id','language','media','tweet_type']\n",
    "NUMERIC_COLUMNS = ['a_follower_count',\n",
    "                     'a_following_count',\n",
    "                     'a_is_verified',\n",
    "                     'b_follower_count',\n",
    "                     'b_following_count',\n",
    "                     'b_is_verified',\n",
    "                     'b_follows_a',\n",
    "                     'tw_len_media',\n",
    "                     'tw_len_photo',\n",
    "                     'tw_len_video',\n",
    "                     'tw_len_gif',\n",
    "                     'tw_len_quest',\n",
    "                     'tw_len_token',\n",
    "                     'tw_count_capital_words',\n",
    "                     'tw_count_excl_quest_marks',\n",
    "                     'tw_count_special1',\n",
    "                     'tw_count_hash',\n",
    "                     'tw_last_quest',\n",
    "                     'tw_len_retweet',\n",
    "                     'tw_len_rt',\n",
    "                     'tw_count_at',\n",
    "                     'tw_count_words',\n",
    "                     'tw_count_char',\n",
    "                     'tw_rt_count_words',\n",
    "                     'tw_rt_count_char',\n",
    "                     'len_hashtags',\n",
    "                     'len_links',\n",
    "                     'len_domains',\n",
    "                     'a_ff_rate',\n",
    "                     'b_ff_rate',\n",
    "                     'ab_fing_rate',\n",
    "                     'ab_fer_rate',\n",
    "                     'a_age',\n",
    "                     'b_age',\n",
    "                     'ab_age_dff',\n",
    "                     'ab_age_rate']\n",
    "len(NUMERIC_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abeb8553-a64f-4c6e-a444-7cc2e01a793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_norm_merge(path, split='train'):\n",
    "    ddf = pd.read_parquet(path)\n",
    "\n",
    "    ddf['quantile'] = 0\n",
    "    quantiles = [92, 216, 442, 1064]\n",
    "    for i, quant in enumerate(quantiles):\n",
    "        ddf['quantile'] = (ddf['quantile']+(ddf['a_follower_count']>quant).astype('int8')).astype('int8')\n",
    "\n",
    "    ddf['date'] = pd.to_datetime(ddf['timestamp'], unit='s')\n",
    "    \n",
    "    VALID_DOW = '2021-02-18'\n",
    "    if split=='train':\n",
    "        ddf = ddf[ddf['date']<pd.to_datetime(VALID_DOW)].reset_index(drop=True)\n",
    "    elif split=='valid':\n",
    "        ddf = ddf[ddf['date']>=pd.to_datetime(VALID_DOW)].reset_index(drop=True)    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    ddf['a_ff_rate'] = (ddf['a_following_count'] / ddf['a_follower_count']).astype('float32')\n",
    "    ddf['b_ff_rate'] = (ddf['b_follower_count']  / ddf['b_following_count']).astype('float32')\n",
    "    ddf['ab_fing_rate'] = (ddf['a_following_count'] / ddf['b_following_count']).astype('float32')\n",
    "    ddf['ab_fer_rate'] = (ddf['a_follower_count'] / (1+ddf['b_follower_count'])).astype('float32')\n",
    "    ddf['a_age'] = ddf['a_account_creation'].astype('int16') + 128\n",
    "    ddf['b_age'] = ddf['b_account_creation'].astype('int16') + 128\n",
    "    ddf['ab_age_dff'] = ddf['b_age'] - ddf['a_age']\n",
    "    ddf['ab_age_rate'] = ddf['a_age']/(1+ddf['b_age'])\n",
    "\n",
    "    ## Normalize\n",
    "    for col in NUMERIC_COLUMNS:\n",
    "        if col == 'tw_len_quest':\n",
    "            ddf[col] = np.clip(ddf[col].values,0,None)\n",
    "        if ddf[col].dtype == 'uint16':\n",
    "            ddf[col].astype('int32')\n",
    "\n",
    "        if col == 'ab_age_dff':\n",
    "            ddf[col] = ddf[col] / 256.            \n",
    "        elif 'int' in str(ddf[col].dtype) or 'float' in str(ddf[col].dtype):    \n",
    "            ddf[col] = np.log1p(ddf[col])\n",
    "\n",
    "        if ddf[col].dtype == 'float64':\n",
    "            ddf[col] = ddf[col].astype('float32') \n",
    "\n",
    "    ## get categorical embedding id        \n",
    "    for col in CAT_COLUMNS:\n",
    "        ddf[col] = ddf[col].astype('float')\n",
    "        if col in ['a_user_id','b_user_id']:\n",
    "            mapping_col = 'a_user_id_b_user_id'\n",
    "        else:\n",
    "            mapping_col = col\n",
    "        mapping = pd.read_parquet(f'/raid/recsys_pre_TE_w_tok/workflow_232parts_joint_thr25/categories/unique.{mapping_col}.parquet').reset_index()\n",
    "        mapping.columns = ['index',col]\n",
    "        ddf = ddf.merge(mapping, how='left', on=col).drop(columns=[col]).rename(columns={'index':col})\n",
    "        ddf[col] = ddf[col].fillna(0).astype('int')        \n",
    "\n",
    "    label_names = ['reply', 'retweet', 'retweet_comment', 'like']\n",
    "    DONT_USE = ['timestamp','a_account_creation','b_account_creation','engage_time',\n",
    "                'fold', 'dt_dow', 'a_account_creation', \n",
    "                'b_account_creation', 'elapsed_time', 'links','domains','hashtags','id', 'date', 'is_train', \n",
    "                'tw_hash0', 'tw_hash1', 'tw_hash2', 'tw_http0', 'tw_uhash', 'tw_hash', 'tw_word0', \n",
    "                'tw_word1', 'tw_word2', 'tw_word3', 'tw_word4', 'dt_minute', 'dt_second',\n",
    "               'dt_day', 'group', 'text', 'tweet_id', 'tw_original_user0', 'tw_original_user1', 'tw_original_user2',\n",
    "                'tw_rt_user0', 'tw_original_http0', 'tw_tweet',]\n",
    "    DONT_USE = [c for c in ddf.columns if c in DONT_USE]\n",
    "    gc.collect(); gc.collect()\n",
    "    \n",
    "    return ddf.drop(columns=DONT_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a8e481-23df-415c-ade3-299add4cdf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATHS = sorted(glob.glob('/raid/recsys/train_proc3/*.parquet'))\n",
    "len(PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f81865-e61a-4565-9d87-24eb576825c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for col in NUMERIC_COLUMNS:\n",
    "#     print(col)\n",
    "#     plt.hist(train[col].values, bins=50)\n",
    "#     plt.title(col)\n",
    "# #     print(ddf[col].describe())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecbabe9b-0254-4619-9f36-bcf30c32da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class AllDataset(Dataset):\n",
    "    def __init__(self, df, max_len_txt, NUMERIC_COLUMNS, CAT_COLUMNS):\n",
    "        self.X = df[NUMERIC_COLUMNS].values\n",
    "        self.X_cat = df[CAT_COLUMNS].values\n",
    "        self.labels = df[label_names].values\n",
    "        self.text_tokens = df.text_tokens.values\n",
    "        self.max_len_txt = max_len_txt\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    def __getitem__(self, index):        \n",
    "#         text = tokenizer.decode([int(token_id) for token_id in self.text_tokens[index][4:-4].split('\\t')]) # [4:-4] is to remove [CLS] and [SEP]\n",
    "#         inputs = tokenizer(text, truncation=True, padding='max_length', max_length=max_len_txt, return_tensors='pt')['input_ids'].squeeze()\n",
    "        inputs = [int(token_id) for token_id in self.text_tokens[index].split('\\t')][:self.max_len_txt]\n",
    "        if len(inputs) < self.max_len_txt:\n",
    "            inputs += [0]*(self.max_len_txt-len(inputs))\n",
    "        return self.X_cat[index], self.X[index].astype(np.float32), torch.tensor(inputs), self.labels[index].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "666aa658-d2f1-424e-8a6d-46b04d4fe7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_dim=128\n",
    "max_len_txt=48\n",
    "emb_dim=768\n",
    "lr = 1e-4 \n",
    "ep = 46   \n",
    "BATCH_SIZE = 1024\n",
    "num_workers = 16\n",
    "use_torch_amp = True\n",
    "import torch.cuda.amp as amp\n",
    "use_amp = False\n",
    "\n",
    "model_name = 'MF_len48_joint_thr25_3weeks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8234130-ef5e-431a-82b4-20d069e97a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NUMERIC_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e34ad1-7f5e-4c9f-b906-77c1cb3c8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 58s, sys: 49.7 s, total: 5min 47s\n",
      "Wall time: 5min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10324907, 47), 10083)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_lst = []\n",
    "for path in PATHS[:10]:\n",
    "    train_lst.append(read_norm_merge(path, 'valid'))\n",
    "valid = pd.concat(train_lst)\n",
    "gc.collect()\n",
    "\n",
    "valid_dataset = AllDataset(valid, max_len_txt, NUMERIC_COLUMNS, CAT_COLUMNS)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers) \n",
    "valid.shape, len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ae69d2c-d12c-4e75-ab23-e4673ff37dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 46, 111, 208, 230,   3,  22, 227, 153,  78,  52,  20, 185,   6,\n",
       "        130, 177,  83,  97, 194,  24, 187,  93,  59, 217, 180, 129,  62,\n",
       "          1,  43, 229, 102, 196,  50,   4,  12, 114,  70,  18,  91,  71,\n",
       "        190, 174,  23,  63,  89, 188,  16, 104,  67,  39, 225, 176,  28,\n",
       "        198,   2,  76, 166, 216, 116, 199, 113, 107, 201,  64, 115,   8,\n",
       "        171,  44, 218, 158, 181,  79,  47, 155, 159, 164, 109,  56, 106,\n",
       "        122, 203, 144,  14, 163, 124, 110, 126,  80,  77,  94, 135,  33,\n",
       "        134, 224, 145, 172, 191,  60, 148, 215, 212, 219,  35, 167,  37,\n",
       "        132, 182, 228,  75,  87, 156, 137,  74,  29,  95, 118,  90, 222,\n",
       "         19,  57, 162, 105, 223, 210, 140,  10,  72, 152, 183, 170,  51,\n",
       "         82, 117,  13, 211, 120,  81, 160,  27, 200, 128, 169, 213, 179,\n",
       "         42,  11, 143,  15, 209, 151,  48, 207, 112, 119, 231, 175,   0,\n",
       "        146, 154,  68, 197,  21, 206, 125, 192,  31,  86, 138,  36, 108,\n",
       "        103,  58, 142,  54,  98,  99, 127, 214,   7,  92, 121, 202, 141,\n",
       "        150,  88,  53,  38, 139, 147, 131,  66,  40,  26, 123,  73, 100,\n",
       "        165, 186, 149, 205,   5, 189,  25,  32, 133, 101, 204, 178, 193,\n",
       "        136,  84, 161,  30, 221,  65,  85,  41,  17,  61,  45, 173, 195,\n",
       "          9, 184,  55,  49, 168,  69,  34,  96, 157, 226, 220]),\n",
       " (232,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_parts_order = np.concatenate([np.random.permutation(232)])\n",
    "train_parts_order = np.array([ 46, 111, 208, 230,   3,  22, 227, 153,  78,  52,  20, 185,   6,\n",
    "        130, 177,  83,  97, 194,  24, 187,  93,  59, 217, 180, 129,  62,\n",
    "          1,  43, 229, 102, 196,  50,   4,  12, 114,  70,  18,  91,  71,\n",
    "        190, 174,  23,  63,  89, 188,  16, 104,  67,  39, 225, 176,  28,\n",
    "        198,   2,  76, 166, 216, 116, 199, 113, 107, 201,  64, 115,   8,\n",
    "        171,  44, 218, 158, 181,  79,  47, 155, 159, 164, 109,  56, 106,\n",
    "        122, 203, 144,  14, 163, 124, 110, 126,  80,  77,  94, 135,  33,\n",
    "        134, 224, 145, 172, 191,  60, 148, 215, 212, 219,  35, 167,  37,\n",
    "        132, 182, 228,  75,  87, 156, 137,  74,  29,  95, 118,  90, 222,\n",
    "         19,  57, 162, 105, 223, 210, 140,  10,  72, 152, 183, 170,  51,\n",
    "         82, 117,  13, 211, 120,  81, 160,  27, 200, 128, 169, 213, 179,\n",
    "         42,  11, 143,  15, 209, 151,  48, 207, 112, 119, 231, 175,   0,\n",
    "        146, 154,  68, 197,  21, 206, 125, 192,  31,  86, 138,  36, 108,\n",
    "        103,  58, 142,  54,  98,  99, 127, 214,   7,  92, 121, 202, 141,\n",
    "        150,  88,  53,  38, 139, 147, 131,  66,  40,  26, 123,  73, 100,\n",
    "        165, 186, 149, 205,   5, 189,  25,  32, 133, 101, 204, 178, 193,\n",
    "        136,  84, 161,  30, 221,  65,  85,  41,  17,  61,  45, 173, 195,\n",
    "          9, 184,  55,  49, 168,  69,  34,  96, 157, 226, 220])\n",
    "train_parts_order, train_parts_order.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c75fba5-0a0b-4512-8edf-d57b61755a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (initial_cat_layer): ConcatenatedEmbeddings(\n",
       "    (embedding_layers): ModuleList(\n",
       "      (0): Embedding(8244536, 128)\n",
       "      (1): Embedding(67, 16)\n",
       "      (2): Embedding(15, 16)\n",
       "      (3): Embedding(4, 16)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (embed): Embedding(119547, 768, padding_idx=0)\n",
       "  (lstm): GRU(768, 128, batch_first=True)\n",
       "  (fn_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=596, out_features=1024, bias=True)\n",
       "      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Swish_Module()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Swish_Module()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Swish_Module()\n",
       "    )\n",
       "  )\n",
       "  (fn_last): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(len(NUMERIC_COLUMNS), layers=[1024,256,64], \n",
    "            embedding_table_shapes={'a_user_id_b_user_id': (8244536, 128), 'language': (67, 16), 'media': (15, 16), 'tweet_type': (4, 16)},\n",
    "            bert_type=bert_type).cuda()\n",
    "\n",
    "for param in model.embed.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b1c59a-cc04-42cc-aad3-872acd35d8af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scaler = amp.GradScaler() if use_torch_amp else None\n",
    "\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, ep-1)\n",
    "scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "rce_best = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce433137-534f-42bf-9c7b-553eb86743c8",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8456096-07c1-4af1-928e-d4a94cb793be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF_len48_joint_thr25_3weeks\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5c46a5-37b6-46ec-937d-ae5f843327df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 00:24:52 2021 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2464, smth: 0.2570: 100%|██████████| 12298/12298 [51:57<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:00<00:00, 83.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 01:19:18 2021 Epoch 1, lr: 0.0001000, train loss: 0.2837, valid loss: 0.2565, mean_rce: 8.31, retweet: 9.04, reply: 11.18, like: 10.27, retweet_comment: 2.74\n",
      "rce_best increased (0.000000 --> 8.305332).  Saving model ...\n",
      "Sun May 30 01:21:23 2021 Epoch: 2\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: CUDA error at: /home/bo/anaconda3/envs/rapids19/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-44657a06242c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx_this_ep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_norm_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-aed468b5cf24>\u001b[0m in \u001b[0;36mread_norm_merge\u001b[0;34m(path, split)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_norm_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mddf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quantile'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m92\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m216\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m442\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1064\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids19/lib/python3.7/site-packages/cudf/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(filepath_or_buffer, engine, columns, filters, row_groups, skiprows, num_rows, strings_to_categorical, use_pandas_metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mnum_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mstrings_to_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrings_to_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0muse_pandas_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_pandas_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         )\n\u001b[1;32m    260\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcudf/_lib/parquet.pyx\u001b[0m in \u001b[0;36mcudf._lib.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcudf/_lib/parquet.pyx\u001b[0m in \u001b[0;36mcudf._lib.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: CUDA error at: /home/bo/anaconda3/envs/rapids19/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, ep+1):\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    scheduler_warmup.step(epoch-1) \n",
    "    \n",
    "    # 5 parts per epoch\n",
    "    idx_this_ep = train_parts_order[(epoch*5-5):epoch*5]\n",
    "    \n",
    "    train_lst = []\n",
    "    for idx in idx_this_ep:\n",
    "        train_lst.append(read_norm_merge(PATHS[idx], 'train' if idx<10 else 'both').to_pandas())\n",
    "    train = pd.concat(train_lst)\n",
    " \n",
    "    gc.collect();gc.collect();\n",
    "    \n",
    "    train_dataset = AllDataset(train, max_len_txt, NUMERIC_COLUMNS, CAT_COLUMNS)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=True) \n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler)\n",
    "    valid_loss,rce,mean_rce = valid_epoch(model, valid_loader)\n",
    "   \n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.4f}, valid loss: {valid_loss:.4f}, mean_rce: {mean_rce:.2f}'\n",
    "    for col in ['retweet', 'reply',  'like', 'retweet_comment']:\n",
    "        content += f', {col}: {rce[col]:.2f}'\n",
    "        \n",
    "    print(content)\n",
    "    \n",
    "    if mean_rce > rce_best:\n",
    "        print('rce_best increased ({:.6f} --> {:.6f}).  Saving model ...'.format(rce_best, mean_rce))\n",
    "        rce_best = mean_rce\n",
    "                \n",
    "        torch.save(model.state_dict(), f'../models/{model_name}_best.pth')\n",
    "        \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict() if scaler else None,      \n",
    "            'rce_best': rce_best,\n",
    "        },\n",
    "        f'../models/{model_name}_last.pth'\n",
    "    )            \n",
    "        \n",
    "torch.save(model.state_dict(), f'../models/{model_name}_final.pth')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16104da-56e5-447e-a441-7b6f63e96fcd",
   "metadata": {},
   "source": [
    "### change to pd loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdfce9df-120f-424d-859d-efc90d3fb5fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "        sd = torch.load(f'../models/{model_name}_last.pth', map_location='cpu')\n",
    "        from_epoch = sd['epoch']\n",
    "        sd['model_state_dict'] = {k[7:] if k.startswith('module.') else k: sd['model_state_dict'][k] for k in sd['model_state_dict'].keys()}\n",
    "        model.load_state_dict(sd['model_state_dict'], strict=True)\n",
    "        optimizer.load_state_dict(sd['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(sd['scaler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4234e63b-02e2-4e56-9923-58bf04914be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 01:38:04 2021 Epoch: 1\n",
      "Sun May 30 01:38:04 2021 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:59<00:00, 48.00s/it]\n",
      "loss: 0.2467, smth: 0.2446: 100%|██████████| 13575/13575 [57:17<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:05<00:00, 80.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 02:41:33 2021 Epoch 2, lr: 0.0010000, train loss: 0.2498, valid loss: 0.2447, mean_rce: 11.53, retweet: 13.58, reply: 13.88, like: 14.72, retweet_comment: 3.94\n",
      "rce_best increased (0.000000 --> 11.530009).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 02:43:40 2021 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:00<00:00, 48.18s/it]\n",
      "loss: 0.2450, smth: 0.2370: 100%|██████████| 13671/13671 [57:44<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:07<00:00, 79.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 03:47:40 2021 Epoch 3, lr: 0.0010000, train loss: 0.2397, valid loss: 0.2374, mean_rce: 13.49, retweet: 16.71, reply: 15.07, like: 17.41, retweet_comment: 4.76\n",
      "rce_best increased (11.530009 --> 13.488159).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 03:49:47 2021 Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:11<00:00, 50.28s/it]\n",
      "loss: 0.2195, smth: 0.2332: 100%|██████████| 14636/14636 [1:01:50<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:07<00:00, 78.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 04:58:04 2021 Epoch 4, lr: 0.0009951, train loss: 0.2337, valid loss: 0.2334, mean_rce: 14.70, retweet: 18.69, reply: 15.56, like: 18.77, retweet_comment: 5.75\n",
      "rce_best increased (13.488159 --> 14.695095).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 05:00:11 2021 Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:08<00:00, 49.60s/it]\n",
      "loss: 0.2316, smth: 0.2293: 100%|██████████| 14331/14331 [1:00:32<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 06:07:08 2021 Epoch 5, lr: 0.0009891, train loss: 0.2298, valid loss: 0.2298, mean_rce: 15.78, retweet: 20.46, reply: 15.97, like: 19.98, retweet_comment: 6.70\n",
      "rce_best increased (14.695095 --> 15.775962).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 06:09:17 2021 Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:44<00:00, 44.92s/it]\n",
      "loss: 0.2366, smth: 0.2271: 100%|██████████| 12543/12543 [53:00<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 07:08:17 2021 Epoch 6, lr: 0.0009806, train loss: 0.2270, valid loss: 0.2261, mean_rce: 16.87, retweet: 21.38, reply: 17.27, like: 21.50, retweet_comment: 7.32\n",
      "rce_best increased (15.775962 --> 16.870298).  Saving model ...\n",
      "Sun May 30 07:10:23 2021 Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:02<00:00, 48.49s/it]\n",
      "loss: 0.2181, smth: 0.2245: 100%|██████████| 13903/13903 [58:43<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 08:15:26 2021 Epoch 7, lr: 0.0009698, train loss: 0.2251, valid loss: 0.2243, mean_rce: 17.45, retweet: 22.12, reply: 17.86, like: 22.15, retweet_comment: 7.68\n",
      "rce_best increased (16.870298 --> 17.450060).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 08:17:33 2021 Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:09<00:00, 49.85s/it]\n",
      "loss: 0.2306, smth: 0.2205: 100%|██████████| 14638/14638 [1:01:57<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:10<00:00, 77.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 09:25:58 2021 Epoch 8, lr: 0.0009568, train loss: 0.2228, valid loss: 0.2225, mean_rce: 18.17, retweet: 23.15, reply: 18.67, like: 22.55, retweet_comment: 8.31\n",
      "rce_best increased (17.450060 --> 18.168774).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 09:28:07 2021 Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:15<00:00, 51.07s/it]\n",
      "loss: 0.2264, smth: 0.2212: 100%|██████████| 14635/14635 [1:01:54<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 10:36:34 2021 Epoch 9, lr: 0.0009415, train loss: 0.2211, valid loss: 0.2204, mean_rce: 18.77, retweet: 23.59, reply: 19.33, like: 23.43, retweet_comment: 8.74\n",
      "rce_best increased (18.168774 --> 18.771233).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 10:38:41 2021 Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:55<00:00, 47.01s/it]\n",
      "loss: 0.2101, smth: 0.2173: 100%|██████████| 13865/13865 [58:22<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 11:43:15 2021 Epoch 10, lr: 0.0009240, train loss: 0.2196, valid loss: 0.2197, mean_rce: 19.19, retweet: 24.07, reply: 19.90, like: 23.50, retweet_comment: 9.28\n",
      "rce_best increased (18.771233 --> 19.188765).  Saving model ...\n",
      "Sun May 30 11:45:23 2021 Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:58<00:00, 47.63s/it]\n",
      "loss: 0.2078, smth: 0.2190: 100%|██████████| 13673/13673 [57:36<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 12:49:13 2021 Epoch 11, lr: 0.0009045, train loss: 0.2186, valid loss: 0.2181, mean_rce: 19.71, retweet: 24.38, reply: 20.46, like: 24.22, retweet_comment: 9.76\n",
      "rce_best increased (19.188765 --> 19.705486).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 12:51:21 2021 Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:03<00:00, 48.77s/it]\n",
      "loss: 0.2238, smth: 0.2156: 100%|██████████| 14359/14359 [1:00:30<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 13:58:11 2021 Epoch 12, lr: 0.0008830, train loss: 0.2173, valid loss: 0.2163, mean_rce: 20.25, retweet: 24.93, reply: 20.96, like: 24.93, retweet_comment: 10.17\n",
      "rce_best increased (19.705486 --> 20.247375).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 14:00:19 2021 Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:56<00:00, 47.24s/it]\n",
      "loss: 0.2033, smth: 0.2154: 100%|██████████| 13714/13714 [57:46<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 15:04:17 2021 Epoch 13, lr: 0.0008597, train loss: 0.2162, valid loss: 0.2156, mean_rce: 20.51, retweet: 25.19, reply: 21.17, like: 25.14, retweet_comment: 10.53\n",
      "rce_best increased (20.247375 --> 20.510010).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 15:06:25 2021 Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:01<00:00, 48.24s/it]\n",
      "loss: 0.2018, smth: 0.2147: 100%|██████████| 14312/14312 [1:00:17<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:07<00:00, 78.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 16:12:59 2021 Epoch 14, lr: 0.0008346, train loss: 0.2153, valid loss: 0.2135, mean_rce: 21.08, retweet: 25.93, reply: 21.43, like: 26.00, retweet_comment: 10.98\n",
      "rce_best increased (20.510010 --> 21.083277).  Saving model ...\n",
      "Sun May 30 16:15:06 2021 Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:05<00:00, 49.03s/it]\n",
      "loss: 0.2253, smth: 0.2138: 100%|██████████| 14636/14636 [1:01:39<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 17:23:07 2021 Epoch 15, lr: 0.0008078, train loss: 0.2143, valid loss: 0.2129, mean_rce: 21.25, retweet: 25.85, reply: 21.68, like: 26.31, retweet_comment: 11.16\n",
      "rce_best increased (21.083277 --> 21.248219).  Saving model ...\n",
      "Sun May 30 17:25:13 2021 Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:00<00:00, 48.18s/it]\n",
      "loss: 0.2282, smth: 0.2123: 100%|██████████| 14522/14522 [1:01:22<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:11<00:00, 76.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 18:32:55 2021 Epoch 16, lr: 0.0007796, train loss: 0.2136, valid loss: 0.2115, mean_rce: 21.69, retweet: 26.39, reply: 22.15, like: 26.83, retweet_comment: 11.41\n",
      "rce_best increased (21.248219 --> 21.692936).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 18:35:01 2021 Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:16<00:00, 51.36s/it]\n",
      "loss: 0.2185, smth: 0.2144: 100%|██████████| 14856/14856 [1:03:05<00:00,  3.92it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 77.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 19:44:40 2021 Epoch 17, lr: 0.0007500, train loss: 0.2128, valid loss: 0.2108, mean_rce: 21.96, retweet: 26.63, reply: 22.46, like: 27.02, retweet_comment: 11.72\n",
      "rce_best increased (21.692936 --> 21.956787).  Saving model ...\n",
      "Sun May 30 19:46:46 2021 Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:13<00:00, 50.62s/it]\n",
      "loss: 0.2067, smth: 0.2110: 100%|██████████| 14637/14637 [1:02:06<00:00,  3.93it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 77.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 20:55:23 2021 Epoch 18, lr: 0.0007192, train loss: 0.2121, valid loss: 0.2099, mean_rce: 22.15, retweet: 26.96, reply: 22.52, like: 27.44, retweet_comment: 11.70\n",
      "rce_best increased (21.956787 --> 22.152912).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 20:57:32 2021 Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:59<00:00, 47.85s/it]\n",
      "loss: 0.2001, smth: 0.2134: 100%|██████████| 13821/13821 [58:35<00:00,  3.93it/s]\n",
      "100%|██████████| 10083/10083 [02:11<00:00, 76.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 22:02:29 2021 Epoch 19, lr: 0.0006873, train loss: 0.2113, valid loss: 0.2093, mean_rce: 22.38, retweet: 27.16, reply: 22.54, like: 27.66, retweet_comment: 12.16\n",
      "rce_best increased (22.152912 --> 22.378281).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 22:04:37 2021 Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:07<00:00, 49.48s/it]\n",
      "loss: 0.2110, smth: 0.2111: 100%|██████████| 14182/14182 [1:00:08<00:00,  3.93it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 77.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 23:11:10 2021 Epoch 20, lr: 0.0006545, train loss: 0.2108, valid loss: 0.2083, mean_rce: 22.74, retweet: 27.20, reply: 23.19, like: 28.12, retweet_comment: 12.44\n",
      "rce_best increased (22.378281 --> 22.737848).  Saving model ...\n",
      "Sun May 30 23:13:16 2021 Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:03<00:00, 48.63s/it]\n",
      "loss: 0.2188, smth: 0.2090: 100%|██████████| 14296/14296 [1:00:38<00:00,  3.93it/s]\n",
      "100%|██████████| 10083/10083 [02:11<00:00, 76.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 00:20:17 2021 Epoch 21, lr: 0.0006210, train loss: 0.2102, valid loss: 0.2076, mean_rce: 22.94, retweet: 27.44, reply: 23.39, like: 28.34, retweet_comment: 12.60\n",
      "rce_best increased (22.737848 --> 22.941046).  Saving model ...\n",
      "Mon May 31 00:22:25 2021 Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:00<00:00, 48.10s/it]\n",
      "loss: 0.2082, smth: 0.2096: 100%|██████████| 13550/13550 [57:11<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:07<00:00, 79.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 01:25:53 2021 Epoch 22, lr: 0.0005868, train loss: 0.2097, valid loss: 0.2067, mean_rce: 23.27, retweet: 27.88, reply: 23.63, like: 28.64, retweet_comment: 12.93\n",
      "rce_best increased (22.941046 --> 23.271320).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 01:28:00 2021 Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:06<00:00, 49.26s/it]\n",
      "loss: 0.2005, smth: 0.2070: 100%|██████████| 14635/14635 [1:01:38<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:07<00:00, 78.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 02:36:00 2021 Epoch 23, lr: 0.0005523, train loss: 0.2092, valid loss: 0.2065, mean_rce: 23.28, retweet: 27.69, reply: 23.64, like: 28.83, retweet_comment: 12.95\n",
      "rce_best increased (23.271320 --> 23.277933).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 02:38:08 2021 Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:00<00:00, 48.00s/it]\n",
      "loss: 0.2226, smth: 0.2096: 100%|██████████| 14180/14180 [59:42<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:06<00:00, 79.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 03:44:05 2021 Epoch 24, lr: 0.0005174, train loss: 0.2086, valid loss: 0.2054, mean_rce: 23.65, retweet: 28.07, reply: 24.08, like: 29.24, retweet_comment: 13.23\n",
      "rce_best increased (23.277933 --> 23.654835).  Saving model ...\n",
      "Mon May 31 03:46:12 2021 Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:55<00:00, 47.10s/it]\n",
      "loss: 0.2100, smth: 0.2069: 100%|██████████| 13862/13862 [58:22<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 04:50:47 2021 Epoch 25, lr: 0.0004826, train loss: 0.2082, valid loss: 0.2049, mean_rce: 23.77, retweet: 28.39, reply: 24.13, like: 29.35, retweet_comment: 13.23\n",
      "rce_best increased (23.654835 --> 23.773729).  Saving model ...\n",
      "Mon May 31 04:52:54 2021 Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:08<00:00, 49.63s/it]\n",
      "loss: 0.2144, smth: 0.2080: 100%|██████████| 14634/14634 [1:01:37<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 06:00:56 2021 Epoch 26, lr: 0.0004477, train loss: 0.2078, valid loss: 0.2046, mean_rce: 23.84, retweet: 28.24, reply: 24.27, like: 29.61, retweet_comment: 13.22\n",
      "rce_best increased (23.773729 --> 23.835400).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 06:03:02 2021 Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:04<00:00, 48.81s/it]\n",
      "loss: 0.2163, smth: 0.2081: 100%|██████████| 14671/14671 [1:01:46<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 07:11:10 2021 Epoch 27, lr: 0.0004132, train loss: 0.2073, valid loss: 0.2039, mean_rce: 24.16, retweet: 28.69, reply: 24.47, like: 29.77, retweet_comment: 13.71\n",
      "rce_best increased (23.835400 --> 24.157763).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 07:13:18 2021 Epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:04<00:00, 48.90s/it]\n",
      "loss: 0.2163, smth: 0.2063: 100%|██████████| 14626/14626 [1:01:34<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 08:21:14 2021 Epoch 28, lr: 0.0003790, train loss: 0.2067, valid loss: 0.2036, mean_rce: 24.27, retweet: 28.76, reply: 24.67, like: 29.84, retweet_comment: 13.80\n",
      "rce_best increased (24.157763 --> 24.268223).  Saving model ...\n",
      "Mon May 31 08:23:20 2021 Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:07<00:00, 49.46s/it]\n",
      "loss: 0.2039, smth: 0.2065: 100%|██████████| 14668/14668 [1:01:44<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 77.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 09:31:29 2021 Epoch 29, lr: 0.0003455, train loss: 0.2065, valid loss: 0.2026, mean_rce: 24.54, retweet: 29.02, reply: 24.85, like: 30.26, retweet_comment: 14.03\n",
      "rce_best increased (24.268223 --> 24.540142).  Saving model ...\n",
      "Mon May 31 09:33:34 2021 Epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:05<00:00, 49.18s/it]\n",
      "loss: 0.2161, smth: 0.2057: 100%|██████████| 14645/14645 [1:01:39<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 10:41:36 2021 Epoch 30, lr: 0.0003127, train loss: 0.2061, valid loss: 0.2021, mean_rce: 24.70, retweet: 29.19, reply: 25.12, like: 30.45, retweet_comment: 14.01\n",
      "rce_best increased (24.540142 --> 24.696213).  Saving model ...\n",
      "Mon May 31 10:43:41 2021 Epoch: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:43<00:00, 44.74s/it]\n",
      "loss: 0.2006, smth: 0.2047: 100%|██████████| 13018/13018 [54:50<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 11:44:31 2021 Epoch 31, lr: 0.0002808, train loss: 0.2056, valid loss: 0.2020, mean_rce: 24.67, retweet: 29.15, reply: 24.98, like: 30.54, retweet_comment: 14.00\n",
      "Mon May 31 11:46:03 2021 Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:57<00:00, 47.51s/it]\n",
      "loss: 0.2135, smth: 0.2046: 100%|██████████| 13865/13865 [58:26<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 77.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 12:50:44 2021 Epoch 32, lr: 0.0002500, train loss: 0.2049, valid loss: 0.2013, mean_rce: 24.95, retweet: 29.52, reply: 25.32, like: 30.75, retweet_comment: 14.21\n",
      "rce_best increased (24.696213 --> 24.949640).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 12:52:49 2021 Epoch: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:01<00:00, 48.32s/it]\n",
      "loss: 0.1938, smth: 0.2051: 100%|██████████| 14470/14470 [1:00:59<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 14:00:07 2021 Epoch 33, lr: 0.0002204, train loss: 0.2048, valid loss: 0.2007, mean_rce: 25.12, retweet: 29.59, reply: 25.49, like: 31.02, retweet_comment: 14.39\n",
      "rce_best increased (24.949640 --> 25.124504).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 14:02:13 2021 Epoch: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:04<00:00, 49.00s/it]\n",
      "loss: 0.2127, smth: 0.2047: 100%|██████████| 14637/14637 [1:01:38<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 15:10:13 2021 Epoch 34, lr: 0.0001922, train loss: 0.2046, valid loss: 0.2004, mean_rce: 25.20, retweet: 29.78, reply: 25.55, like: 31.07, retweet_comment: 14.38\n",
      "rce_best increased (25.124504 --> 25.196066).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 15:12:20 2021 Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:04<00:00, 48.84s/it]\n",
      "loss: 0.1989, smth: 0.2023: 100%|██████████| 14635/14635 [1:01:35<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 16:20:17 2021 Epoch 35, lr: 0.0001654, train loss: 0.2042, valid loss: 0.1998, mean_rce: 25.45, retweet: 29.85, reply: 25.68, like: 31.38, retweet_comment: 14.86\n",
      "rce_best increased (25.196066 --> 25.445557).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 16:22:21 2021 Epoch: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:59<00:00, 47.83s/it]\n",
      "loss: 0.2033, smth: 0.2039: 100%|██████████| 13586/13586 [57:15<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:08<00:00, 78.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 17:25:52 2021 Epoch 36, lr: 0.0001403, train loss: 0.2039, valid loss: 0.1996, mean_rce: 25.48, retweet: 29.87, reply: 25.80, like: 31.44, retweet_comment: 14.81\n",
      "rce_best increased (25.445557 --> 25.481163).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 17:27:57 2021 Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:03<00:00, 48.66s/it]\n",
      "loss: 0.2083, smth: 0.2044: 100%|██████████| 14526/14526 [1:01:14<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 77.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 18:35:31 2021 Epoch 37, lr: 0.0001170, train loss: 0.2035, valid loss: 0.1990, mean_rce: 25.66, retweet: 30.13, reply: 25.85, like: 31.66, retweet_comment: 15.02\n",
      "rce_best increased (25.481163 --> 25.664833).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 18:37:38 2021 Epoch: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:06<00:00, 49.27s/it]\n",
      "loss: 0.1996, smth: 0.2025: 100%|██████████| 14636/14636 [1:01:42<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:10<00:00, 77.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 19:45:46 2021 Epoch 38, lr: 0.0000955, train loss: 0.2034, valid loss: 0.1991, mean_rce: 25.70, retweet: 30.18, reply: 26.05, like: 31.56, retweet_comment: 15.01\n",
      "rce_best increased (25.664833 --> 25.700359).  Saving model ...\n",
      "Mon May 31 19:47:52 2021 Epoch: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:05<00:00, 49.05s/it]\n",
      "loss: 0.2044, smth: 0.2017: 100%|██████████| 14637/14637 [1:01:39<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 20:55:54 2021 Epoch 39, lr: 0.0000760, train loss: 0.2034, valid loss: 0.1989, mean_rce: 25.72, retweet: 30.13, reply: 26.03, like: 31.71, retweet_comment: 15.02\n",
      "rce_best increased (25.700359 --> 25.722523).  Saving model ...\n",
      "Mon May 31 20:57:59 2021 Epoch: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:55<00:00, 47.15s/it]\n",
      "loss: 0.2048, smth: 0.2020: 100%|██████████| 13508/13508 [56:55<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 22:01:06 2021 Epoch 40, lr: 0.0000585, train loss: 0.2026, valid loss: 0.1983, mean_rce: 25.92, retweet: 30.35, reply: 26.28, like: 31.94, retweet_comment: 15.10\n",
      "rce_best increased (25.722523 --> 25.916914).  Saving model ...\n",
      "Mon May 31 22:03:15 2021 Epoch: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:07<00:00, 49.55s/it]\n",
      "loss: 0.1972, smth: 0.2034: 100%|██████████| 14637/14637 [1:01:40<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 23:11:20 2021 Epoch 41, lr: 0.0000432, train loss: 0.2029, valid loss: 0.1983, mean_rce: 25.95, retweet: 30.32, reply: 26.33, like: 31.92, retweet_comment: 15.24\n",
      "rce_best increased (25.916914 --> 25.952074).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 31 23:13:27 2021 Epoch: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:00<00:00, 48.07s/it]\n",
      "loss: 0.1962, smth: 0.2038: 100%|██████████| 14473/14473 [1:00:59<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 78.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 00:20:43 2021 Epoch 42, lr: 0.0000302, train loss: 0.2027, valid loss: 0.1980, mean_rce: 26.01, retweet: 30.44, reply: 26.35, like: 32.02, retweet_comment: 15.21\n",
      "rce_best increased (25.952074 --> 26.005121).  Saving model ...\n",
      "Tue Jun  1 00:22:49 2021 Epoch: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:53<00:00, 46.71s/it]\n",
      "loss: 0.1964, smth: 0.2036: 100%|██████████| 14187/14187 [59:45<00:00,  3.96it/s]\n",
      "100%|██████████| 10083/10083 [02:09<00:00, 77.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 01:28:46 2021 Epoch 43, lr: 0.0000194, train loss: 0.2026, valid loss: 0.1980, mean_rce: 26.05, retweet: 30.51, reply: 26.33, like: 32.02, retweet_comment: 15.32\n",
      "rce_best increased (26.005121 --> 26.048130).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 01:30:52 2021 Epoch: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:00<00:00, 48.14s/it]\n",
      "loss: 0.2028, smth: 0.2035: 100%|██████████| 14784/14784 [1:02:35<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:10<00:00, 77.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 02:39:47 2021 Epoch 44, lr: 0.0000109, train loss: 0.2026, valid loss: 0.1979, mean_rce: 26.06, retweet: 30.47, reply: 26.38, like: 32.05, retweet_comment: 15.31\n",
      "rce_best increased (26.048130 --> 26.055038).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 02:41:56 2021 Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:54<00:00, 46.84s/it]\n",
      "loss: 0.2097, smth: 0.2018: 100%|██████████| 13823/13823 [58:26<00:00,  3.94it/s]\n",
      "100%|██████████| 10083/10083 [02:10<00:00, 77.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 03:46:35 2021 Epoch 45, lr: 0.0000049, train loss: 0.2024, valid loss: 0.1980, mean_rce: 26.05, retweet: 30.50, reply: 26.39, like: 32.02, retweet_comment: 15.30\n",
      "Tue Jun  1 03:48:07 2021 Epoch: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:02<00:00, 48.46s/it]\n",
      "loss: 0.2023, smth: 0.2006: 100%|██████████| 14635/14635 [1:01:45<00:00,  3.95it/s]\n",
      "100%|██████████| 10083/10083 [02:10<00:00, 77.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 04:56:12 2021 Epoch 46, lr: 0.0000012, train loss: 0.2025, valid loss: 0.1979, mean_rce: 26.04, retweet: 30.50, reply: 26.37, like: 32.05, retweet_comment: 15.24\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, ep+1):\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    scheduler_warmup.step(epoch-1) \n",
    "    \n",
    "    if epoch<=1: continue\n",
    "    \n",
    "    # 5 parts per epoch\n",
    "    idx_this_ep = train_parts_order[(epoch*5-5):epoch*5]\n",
    "    \n",
    "    train_lst = []\n",
    "    for idx in tqdm(idx_this_ep):\n",
    "        train_lst.append(read_norm_merge(PATHS[idx], 'train' if idx<10 else 'both'))\n",
    "    train = pd.concat(train_lst)\n",
    " \n",
    "    gc.collect();gc.collect();\n",
    "    \n",
    "    train_dataset = AllDataset(train, max_len_txt, NUMERIC_COLUMNS, CAT_COLUMNS)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=True) \n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler)\n",
    "    valid_loss,rce,mean_rce = valid_epoch(model, valid_loader)\n",
    "   \n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.4f}, valid loss: {valid_loss:.4f}, mean_rce: {mean_rce:.2f}'\n",
    "    for col in ['retweet', 'reply',  'like', 'retweet_comment']:\n",
    "        content += f', {col}: {rce[col]:.2f}'\n",
    "        \n",
    "    print(content)\n",
    "    \n",
    "    if mean_rce > rce_best:\n",
    "        print('rce_best increased ({:.6f} --> {:.6f}).  Saving model ...'.format(rce_best, mean_rce))\n",
    "        rce_best = mean_rce\n",
    "                \n",
    "        torch.save(model.state_dict(), f'../models/{model_name}_best.pth')\n",
    "        \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict() if scaler else None,      \n",
    "            'rce_best': rce_best,\n",
    "        },\n",
    "        f'../models/{model_name}_last.pth'\n",
    "    )            \n",
    "        \n",
    "torch.save(model.state_dict(), f'../models/{model_name}_final.pth')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18453f2a-675a-4df6-aade-eac0f13cd9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e79b7-63aa-47a3-a895-d8b8b2a5a8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be1d69-d858-4f0e-a876-b80df6730af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 parts, 5 epochs\n",
    "mean_rce: 16.51, retweet: 20.27, reply: 18.26, like: 20.17, retweet_comment: 7.35\n",
    "                    \n",
    "Epoch 20, lr: 0.0000794, train loss: 0.2142, valid loss: 0.2233, \n",
    "mean_rce: 18.66, retweet: 23.00, reply: 20.36, like: 21.72, retweet_comment: 9.56     \n",
    "                    \n",
    "Epoch 35, lr: 0.0001654, train loss: 0.2105, valid loss: 0.2220, \n",
    "mean_rce: 19.25, retweet: 23.58, reply: 21.08, like: 22.09, retweet_comment: 10.27                    \n",
    "\n",
    "# xgb feat NN                    \n",
    "mean_rce: 20.25, retweet: 23.39, reply: 19.07, like: 13.02, retweet_comment: 25.54                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d62e2-0bdf-4000-8be4-05e8ceb334d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bab4aa-0ee6-420c-bfa3-7d8272abb170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "204f9dd2-4f73-4f1c-91a4-eb63c4200f53",
   "metadata": {},
   "source": [
    "## load best ep and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f33716f-3e81-437c-816b-945cbf692100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = torch.load(f'../models/{model_name}_best.pth')\n",
    "sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "model.load_state_dict(sd, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b4798bb-75bc-4cee-b7cf-38405626f46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'reply', 'retweet', 'retweet_comment']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = sorted(label_names)\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0ca8010-67b1-48c8-947f-80b5cb38ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10083/10083 [01:43<00:00, 97.81it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.13601"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = []\n",
    "LOGITS = []\n",
    "TARGETS = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_loader):\n",
    "        x_cat, x_cont, text_tok, targets = batch\n",
    "        x_cat = x_cat.cuda()     \n",
    "        x_cont = x_cont.cuda()\n",
    "        text_tok = text_tok.cuda()\n",
    "        targets = targets.cuda()            \n",
    "        logits = model(x_cat, x_cont, text_tok)\n",
    "        loss = criterion(logits, targets)\n",
    "        val_loss.append(loss.item())\n",
    "        LOGITS.append(logits.cpu())\n",
    "        TARGETS.append(targets.cpu())\n",
    "\n",
    "LOGITS = torch.cat(LOGITS)\n",
    "TARGETS = torch.cat(TARGETS)\n",
    "rce = {}\n",
    "for i in range(4):\n",
    "    rce[label_names[i]] = compute_rce_fast(cp.asarray(LOGITS[:,i].sigmoid()),cp.asarray(TARGETS[:,i])).get()            \n",
    "mean_rce = np.mean([v for k,v in rce.items()])\n",
    "mean_rce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c20580d0-1ab5-44ff-83c0-c8f854c32e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_quantile = pd.concat([pd.read_parquet(path)[['quantile']] for path in VALID_PATHS]).reset_index(drop=True)\n",
    "# df_quantile = df_quantile.apply(np.expm1).round().astype(int)\n",
    "df_quantile = valid[['quantile']].copy().reset_index(drop=True)\n",
    "df_quantile.shape\n",
    "\n",
    "yquantile = cupy.asarray(df_quantile.values)\n",
    "oof = cupy.asarray(LOGITS.sigmoid())\n",
    "yvalid = cupy.asarray(TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed639820-f204-40fa-9e06-917e86143130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import compute_prauc, average_precision_score,display_score\n",
    "\n",
    "rce_output = {}\n",
    "ap_output = {}\n",
    "for i in range(4):\n",
    "    prauc_out = []\n",
    "    rce_out = []\n",
    "    ap_out = []\n",
    "    for j in range(5):\n",
    "        this_quantile_idx = (df_quantile == j)['quantile'].values\n",
    "        yvalid_tmp = yvalid[this_quantile_idx][:, i]\n",
    "        oof_tmp = oof[this_quantile_idx][:, i]\n",
    "        prauc = compute_prauc(oof_tmp, yvalid_tmp)\n",
    "        rce   = compute_rce_fast(oof_tmp, yvalid_tmp).item()\n",
    "        ap    = average_precision_score(cupy.asnumpy(yvalid_tmp),cupy.asnumpy(oof_tmp))\n",
    "        prauc_out.append(prauc)\n",
    "        rce_out.append(rce)\n",
    "        ap_out.append(ap)\n",
    "    rce_output[label_names[i]] = rce_out\n",
    "    ap_output[label_names[i]] = ap_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06116e4d-9b72-4b21-abda-4a3b90811548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_cat5_cont36_frzemb768_gru128_len64_thr50_3weeks\n",
      "Quantile Group|AP Retweet|RCE Retweet|  AP Reply|  RCE Reply|   AP Like|   RCE Like|AP RT comment|RCE RT comment\n",
      "        0          0.4827     25.3136     0.2257     18.1147     0.7328     17.5110     0.0545      9.7572\n",
      "        1          0.4643     24.6773     0.2055     18.0764     0.7295     17.6294     0.0554     10.1372\n",
      "        2          0.4435     23.6253     0.2155     18.7955     0.7326     18.1288     0.0489      8.8695\n",
      "        3          0.4352     23.1426     0.2309     19.5610     0.7339     18.9700     0.0433      8.8682\n",
      "        4          0.4320     24.3085     0.2189     21.0982     0.7610     27.1258     0.0494     11.0005\n",
      "     Average       0.4515     24.2134     0.2193     19.1291     0.7379     19.8730     0.0503      9.7265\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "display_score(rce_output, ap_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50140035-bee5-4f09-beb1-a7b4868d9b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "687aee60-5926-48c7-af86-8fc25709ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Group|AP Retweet|RCE Retweet|  AP Reply|  RCE Reply|   AP Like|   RCE Like|AP RT comment|RCE RT comment\n",
      "        0          0.4767     23.8403     0.2466     19.7694     0.7496     18.7935     0.0803     11.3031\n",
      "        1          0.4608     24.3840     0.2391     20.9897     0.7444     20.0392     0.0680     10.6829\n",
      "        2          0.4501     24.8955     0.2513     22.2341     0.7393     20.2754     0.0682     11.2699\n",
      "        3          0.4384     24.5673     0.2677     23.6996     0.7334     20.9961     0.0662     11.5356\n",
      "        4          0.4124     24.7017     0.2411     24.6334     0.7059     20.6678     0.0710     15.1545\n",
      "     Average       0.4477     24.4778     0.2492     22.2652     0.7345     20.1544     0.0707     11.9892\n",
      "CPU times: user 0 ns, sys: 1 ms, total: 1 ms\n",
      "Wall time: 950 µs\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "%%time\n",
    "display_score(rce_output, ap_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75beb1f4-6b36-4b3e-ba4a-f7d5507e9452",
   "metadata": {},
   "source": [
    "## load best ep and inference LB valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6734fb9-a270-4a4a-940f-c6fb2c1c0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_norm_merge(ddf):\n",
    "\n",
    "    ddf['quantile'] = 0\n",
    "    quantiles = [ 240,  588, 1331, 3996]\n",
    "    for i, quant in enumerate(quantiles):\n",
    "        ddf['quantile'] = (ddf['quantile']+(ddf['a_follower_count']>quant).astype('int8')).astype('int8')\n",
    "\n",
    "    ddf['date'] = cudf.to_datetime(ddf['timestamp'], unit='s')\n",
    "       \n",
    "    ddf['a_ff_rate'] = (ddf['a_following_count'] / ddf['a_follower_count']).astype('float32')\n",
    "    ddf['b_ff_rate'] = (ddf['b_follower_count']  / ddf['b_following_count']).astype('float32')\n",
    "    ddf['ab_fing_rate'] = (ddf['a_following_count'] / ddf['b_following_count']).astype('float32')\n",
    "    ddf['ab_fer_rate'] = (ddf['a_follower_count'] / (1+ddf['b_follower_count'])).astype('float32')\n",
    "    ddf['a_age'] = ddf['a_account_creation'].astype('int16') + 128\n",
    "    ddf['b_age'] = ddf['b_account_creation'].astype('int16') + 128\n",
    "    ddf['ab_age_dff'] = ddf['b_age'] - ddf['a_age']\n",
    "    ddf['ab_age_rate'] = ddf['a_age']/(1+ddf['b_age'])\n",
    "\n",
    "    ## Normalize\n",
    "    for col in NUMERIC_COLUMNS:\n",
    "        if col == 'tw_len_quest':\n",
    "            ddf[col] = np.clip(ddf[col].values.get(),0,None)\n",
    "        if ddf[col].dtype == 'uint16':\n",
    "            ddf[col].astype('int32')\n",
    "\n",
    "        if col == 'ab_age_dff':\n",
    "            ddf[col] = ddf[col] / 256.            \n",
    "        elif 'int' in str(ddf[col].dtype) or 'float' in str(ddf[col].dtype):    \n",
    "            ddf[col] = np.log1p(ddf[col])\n",
    "\n",
    "        if ddf[col].dtype == 'float64':\n",
    "            ddf[col] = ddf[col].astype('float32') \n",
    "\n",
    "    ## get categorical embedding id        \n",
    "    for col in CAT_COLUMNS:\n",
    "        ddf[col] = ddf[col].astype('float')\n",
    "        if col in ['a_user_id','b_user_id']:\n",
    "            mapping_col = 'a_user_id_b_user_id'\n",
    "        else:\n",
    "            mapping_col = col\n",
    "        mapping = cudf.read_parquet(f'/raid/recsys_pre_TE_w_tok/workflow_232parts_joint_thr25/categories/unique.{mapping_col}.parquet').reset_index()\n",
    "        mapping.columns = ['index',col]\n",
    "        ddf = ddf.merge(mapping, how='left', on=col).drop(columns=[col]).rename(columns={'index':col})\n",
    "        ddf[col] = ddf[col].fillna(0).astype('int')        \n",
    "\n",
    "    label_names = ['reply', 'retweet', 'retweet_comment', 'like']\n",
    "    DONT_USE = ['timestamp','a_account_creation','b_account_creation','engage_time',\n",
    "                'fold', 'dt_dow', 'a_account_creation', \n",
    "                'b_account_creation', 'elapsed_time', 'links','domains','hashtags','id', 'date', 'is_train', \n",
    "                'tw_hash0', 'tw_hash1', 'tw_hash2', 'tw_http0', 'tw_uhash', 'tw_hash', 'tw_word0', \n",
    "                'tw_word1', 'tw_word2', 'tw_word3', 'tw_word4', 'dt_minute', 'dt_second',\n",
    "               'dt_day', 'group', 'text', 'tweet_id', 'tw_original_user0', 'tw_original_user1', 'tw_original_user2',\n",
    "                'tw_rt_user0', 'tw_original_http0', 'tw_tweet',]\n",
    "    DONT_USE = [c for c in ddf.columns if c in DONT_USE]\n",
    "    gc.collect(); gc.collect()\n",
    "    \n",
    "    return ddf.drop(columns=DONT_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6158581a-dd35-405a-9bb0-9f2b7dba264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 10.5 s, total: 27.9 s\n",
      "Wall time: 30 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14461760, 47), 14123)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df = cudf.read_parquet('/raid/recsys_valid/valid_proc.parquet',num_rows=7_000_000)\n",
    "df = read_norm_merge(df).to_pandas()\n",
    "\n",
    "df2 = cudf.read_parquet('/raid/recsys_valid/valid_proc.parquet',skiprows=7_000_000)\n",
    "df2 = read_norm_merge(df2).to_pandas()\n",
    "\n",
    "valid = pd.concat([df,df2])\n",
    "del df,df2\n",
    "gc.collect()\n",
    "\n",
    "valid_dataset = AllDataset(valid, max_len_txt, NUMERIC_COLUMNS, CAT_COLUMNS)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers) \n",
    "valid.shape, len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d7239ea-c307-446c-bc5a-811c5f79049c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sd = torch.load(f'../models/{model_name}_best.pth')\n",
    "sd = torch.load('/home/bo/kaggle/recsys/recsysChallenge2021/bo/sub/v11_len48_thr25_joint_MF/MF_len48_joint_thr25_3weeks_best.pth')\n",
    "sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "model.load_state_dict(sd, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d2653d-0dd2-404b-874e-0ba5a041f980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'reply', 'retweet', 'retweet_comment']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = sorted(label_names)\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e195affe-8799-44b6-8f92-186e86d86e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14123/14123 [02:46<00:00, 84.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.316917"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = []\n",
    "LOGITS = []\n",
    "TARGETS = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_loader):\n",
    "        x_cat, x_cont, text_tok, targets = batch\n",
    "        x_cat = x_cat.cuda()     \n",
    "        x_cont = x_cont.cuda()\n",
    "        text_tok = text_tok.cuda()\n",
    "        targets = targets.cuda()            \n",
    "        logits = model(x_cat, x_cont, text_tok)\n",
    "        loss = criterion(logits, targets)\n",
    "        val_loss.append(loss.item())\n",
    "        LOGITS.append(logits.cpu())\n",
    "        TARGETS.append(targets.cpu())\n",
    "\n",
    "LOGITS = torch.cat(LOGITS)\n",
    "TARGETS = torch.cat(TARGETS)\n",
    "rce = {}\n",
    "for i in range(4):\n",
    "    rce[label_names[i]] = compute_rce_fast(cp.asarray(LOGITS[:,i].sigmoid()),cp.asarray(TARGETS[:,i])).get()            \n",
    "mean_rce = np.mean([v for k,v in rce.items()])\n",
    "mean_rce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db87e4c-e82f-46d6-8d99-11da8385bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_quantile = pd.concat([pd.read_parquet(path)[['quantile']] for path in VALID_PATHS]).reset_index(drop=True)\n",
    "# df_quantile = df_quantile.apply(np.expm1).round().astype(int)\n",
    "df_quantile = valid[['quantile']].copy().reset_index(drop=True)\n",
    "df_quantile.shape\n",
    "\n",
    "yquantile = cupy.asarray(df_quantile.values)\n",
    "oof = cupy.asarray(LOGITS.sigmoid())\n",
    "yvalid = cupy.asarray(TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6fa3362-eca0-4e82-95f8-1f5fc7d90114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import compute_prauc, average_precision_score,display_score\n",
    "\n",
    "rce_output = {}\n",
    "ap_output = {}\n",
    "for i in range(4):\n",
    "    prauc_out = []\n",
    "    rce_out = []\n",
    "    ap_out = []\n",
    "    for j in range(5):\n",
    "        this_quantile_idx = (df_quantile == j)['quantile'].values\n",
    "        yvalid_tmp = yvalid[this_quantile_idx][:, i]\n",
    "        oof_tmp = oof[this_quantile_idx][:, i]\n",
    "        prauc = compute_prauc(oof_tmp, yvalid_tmp)\n",
    "        rce   = compute_rce_fast(oof_tmp, yvalid_tmp).item()\n",
    "        ap    = average_precision_score(cupy.asnumpy(yvalid_tmp),cupy.asnumpy(oof_tmp))\n",
    "        prauc_out.append(prauc)\n",
    "        rce_out.append(rce)\n",
    "        ap_out.append(ap)\n",
    "    rce_output[label_names[i]] = rce_out\n",
    "    ap_output[label_names[i]] = ap_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83515771-3450-4d9b-9f1a-48e6ac1f24d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF_len48_joint_thr25_3weeks\n",
      "Quantile Group|AP Retweet|RCE Retweet|  AP Reply|  RCE Reply|   AP Like|   RCE Like|AP RT comment|RCE RT comment\n",
      "        0          0.3648     19.1185     0.1768     17.4406     0.5983      8.8159     0.0343      8.4536\n",
      "        1          0.3457     18.4016     0.1811     17.8598     0.5753      6.9555     0.0309      8.3305\n",
      "        2          0.3388     17.9507     0.2000     18.7393     0.5646      6.6237     0.0306      8.0775\n",
      "        3          0.3504     17.6180     0.2199     19.9633     0.5772      7.1854     0.0310      8.4171\n",
      "        4          0.3247     16.5028     0.1267     14.6986     0.6501     10.9945     0.0286      8.7294\n",
      "     Average       0.3449     17.9183     0.1809     17.7403     0.5931      8.1150     0.0311      8.4016\n"
     ]
    }
   ],
   "source": [
    "# public test\n",
    "print(model_name)\n",
    "display_score(rce_output, ap_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3df9b-164d-4c63-955a-ce2c4d53283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boliu0\tsub_0601   0.3446\t17.9344     \t0.1829\t17.8545\t    0.5926\t    8.0979\t    0.0314\t   8.4308\t9 hours\t258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ded2436d-3e40-4436-a482-6d1aef00a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF_len48_joint_thr25_3weeks\n",
      "Quantile Group|AP Retweet|RCE Retweet|  AP Reply|  RCE Reply|   AP Like|   RCE Like|AP RT comment|RCE RT comment\n",
      "        0          0.5608     32.5540     0.2911     23.4060     0.8070     28.8335     0.0951     15.3339\n",
      "        1          0.5250     30.9199     0.2968     24.7680     0.8037     29.8295     0.0780     14.2486\n",
      "        2          0.5138     30.4446     0.3222     26.5348     0.7998     30.7027     0.0694     14.0940\n",
      "        3          0.5267     31.0953     0.3355     27.6502     0.8038     31.8786     0.0766     15.1034\n",
      "        4          0.4854     29.5893     0.2521     24.8843     0.7972     33.1890     0.0790     15.7639\n",
      "     Average       0.5223     30.9206     0.2995     25.4487     0.8023     30.8866     0.0797     14.9087\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "display_score(rce_output, ap_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e644dcce-bb1b-4c3b-b3e6-37ddd6a3eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF_len48_joint_thr25_3weeks\n",
      "Quantile Group|AP Retweet|RCE Retweet|  AP Reply|  RCE Reply|   AP Like|   RCE Like|AP RT comment|RCE RT comment\n",
      "        0          0.5756     33.0314     0.2987     22.8756     0.8081     28.1401     0.0998     15.3117\n",
      "        1          0.5540     32.3156     0.2850     23.5546     0.8063     29.1117     0.0939     15.5322\n",
      "        2          0.5309     31.2906     0.2963     24.6103     0.8050     29.6993     0.0826     14.4356\n",
      "        3          0.5168     30.4555     0.3086     25.7036     0.8003     30.3232     0.0723     14.2019\n",
      "        4          0.4954     29.9846     0.2819     26.2819     0.7986     32.8644     0.0774     15.5282\n",
      "     Average       0.5345     31.4155     0.2941     24.6052     0.8037     30.0277     0.0852     15.0019\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "display_score(rce_output, ap_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
