{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a257133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA CORPORATION\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450b23af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\" \n",
    "\n",
    "import matplotlib.pyplot as plt, time\n",
    "import tensorflow as tf\n",
    "import cudf, numpy as np, pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import cupy, gc\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4a004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1c9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "#print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c8e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 86\n",
    "MODEL_NUM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3788fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL OF GIBA PREPROCESS FEATURES\n",
    "ALL = ['hashtags', 'tweet_id', 'media', 'links', 'domains', 'tweet_type',\n",
    "       'language', 'timestamp', 'a_user_id', 'a_follower_count',\n",
    "       'a_following_count', 'a_is_verified', 'a_account_creation', 'b_user_id',\n",
    "       'b_follower_count', 'b_following_count', 'b_is_verified',\n",
    "       'b_account_creation', 'b_follows_a', 'reply', 'retweet',\n",
    "       'retweet_comment', 'like', 'text', 'tw_len_media', 'tw_len_photo',\n",
    "       'tw_len_video', 'tw_len_gif', 'tw_len_quest', 'tw_len_token',\n",
    "       'tw_count_capital_words', 'tw_count_excl_quest_marks',\n",
    "       'tw_count_special1', 'tw_count_hash', 'tw_last_quest', 'tw_len_retweet',\n",
    "       'tw_len_rt', 'tw_count_at', 'tw_count_words', 'tw_count_char',\n",
    "       'tw_rt_count_words', 'tw_rt_count_char', 'tw_original_user0',\n",
    "       'tw_original_user1', 'tw_original_user2', 'tw_rt_user0',\n",
    "       'tw_original_http0', 'tw_word0', 'tw_word1', 'tw_word2', 'tw_word3',\n",
    "       'tw_word4', 'tw_tweet', 'group', 'dt_day', 'dt_dow', 'dt_minute',\n",
    "       'len_hashtags', 'len_links', 'len_domains']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858a680",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6653ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATS = ['b_follows_a','a_follower_count','a_following_count','b_follower_count','b_following_count'] #,\n",
    "#         'a_account_creation','b_account_creation',\n",
    "#         'a_is_verified','b_is_verified']\n",
    "\n",
    "FEATS += ['dt_dow', 'dt_minute',\n",
    "          'len_hashtags', 'len_links', 'len_domains']\n",
    "\n",
    "FEATS += ['tw_len_media', 'tw_len_photo','tw_len_video', 'tw_len_gif', \n",
    "          'tw_len_quest', 'tw_len_token',\n",
    "          'tw_count_capital_words', 'tw_count_excl_quest_marks',\n",
    "          'tw_count_special1', 'tw_count_hash', 'tw_last_quest', \n",
    "          'tw_len_retweet', 'tw_len_rt', 'tw_count_at', 'tw_count_words', 'tw_count_char',\n",
    "          'tw_rt_count_words', 'tw_rt_count_char']\n",
    "\n",
    "FEATS2 = ['a_user_id', 'b_user_id','tweet_type','language','media']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab003353",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARS = ['reply', 'retweet','retweet_comment', 'like']\n",
    "COLS = TARS + FEATS2 + FEATS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ece69",
   "metadata": {},
   "source": [
    "# Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240288a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 96\n",
    "EMB_SIZE2 = 96*2\n",
    "TOK_SIZE = 48\n",
    "\n",
    "def build_model():\n",
    "    inp = tf.keras.layers.Input(shape=(len(FEATURES),))\n",
    "    inp2 = tf.keras.layers.Input(shape=(TOK_SIZE,))\n",
    "\n",
    "    embeds = []\n",
    "    embeds.append( tf.keras.layers.Embedding(len(IDX)+1,EMB_SIZE) ) # USER_ID\n",
    "    \n",
    "    embeds.append( tf.keras.layers.Embedding(3,2) ) # TWEET_TYPE\n",
    "    embeds.append( tf.keras.layers.Embedding(66,10) ) # LANGUAGE\n",
    "    embeds.append( tf.keras.layers.Embedding(14,4) ) # MEDIA\n",
    "    embeds.append( tf.keras.layers.Embedding(7,4) ) # DT_DOW\n",
    "       \n",
    "    # USERS\n",
    "    a_user = embeds[0](inp[:,0])\n",
    "    b_user = embeds[0](inp[:,1])\n",
    "    \n",
    "    # USER INTERACTION\n",
    "    a_embed = tf.keras.layers.Concatenate()([a_user,inp[:,-len(NORM_FEATS):-len(NORM_FEATS)+2],inp[:,-1:]])\n",
    "    a_embed = tf.keras.layers.Dense(EMB_SIZE,activation='tanh')(a_embed)\n",
    "    a_dot_b = tf.keras.layers.Dot(axes=-1,normalize=True)([a_embed,b_user])\n",
    "        \n",
    "    # CAT FEATURE EMBEDDINGS\n",
    "    embeds2 = []    \n",
    "    for k in range(2,len(CAT_FEATS)):\n",
    "        embeds2.append( embeds[k-1](inp[:,k]) )\n",
    "    x1 = tf.keras.layers.Concatenate()(embeds2)\n",
    "        \n",
    "    # TWEET TOKEN EMBEDDINGS\n",
    "    embeds3 = []\n",
    "    word_emb = tf.keras.layers.Embedding(119548,EMB_SIZE2)\n",
    "    for k in range(TOK_SIZE):\n",
    "        embeds3.append( word_emb(inp2[:,k]) )\n",
    "    x2 = tf.keras.layers.Average()(embeds3)\n",
    "    \n",
    "    # USER INTERACT WITH TWEET\n",
    "    tweet_embed = tf.keras.layers.Concatenate()([x1,x2,inp[:,-len(NORM_FEATS):-3]])\n",
    "    tweet_embed = tf.keras.layers.Dense(EMB_SIZE,activation='tanh')(tweet_embed)\n",
    "    b_dot_tweet = tf.keras.layers.Dot(axes=-1,normalize=True)([tweet_embed,b_user])\n",
    "\n",
    "    # NUMERICAL FEATURES\n",
    "    x = tf.keras.layers.Concatenate()(\n",
    "        [a_user,b_user,a_dot_b,b_dot_tweet,x1,x2,inp[:,-len(NORM_FEATS):]])\n",
    "    \n",
    "    HIDDEN_SIZE = 256+64\n",
    "    LAYERS = 3\n",
    "    \n",
    "    for k in range(LAYERS):\n",
    "        x = tf.keras.layers.Dense(HIDDEN_SIZE)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    # CONCAT WITH MATRIX FACTORIZATION\n",
    "    x = tf.keras.layers.Concatenate()([a_dot_b,b_dot_tweet,x])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(4,activation='sigmoid',dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs=[inp,inp2],outputs=x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422ccff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['a_user_id', 'b_user_id', 'tweet_type', 'language', 'media',\n",
    "       'dt_dow', 'NORM_LOG_a_follower_count',\n",
    "       'NORM_LOG_a_following_count', 'NORM_dt_minute',\n",
    "       'NORM_len_hashtags', 'NORM_len_links', 'NORM_len_domains',\n",
    "       'NORM_tw_len_media', 'NORM_tw_len_photo', 'NORM_tw_len_video',\n",
    "       'NORM_tw_len_gif', 'NORM_tw_len_quest', 'NORM_tw_len_token',\n",
    "       'NORM_tw_count_capital_words', 'NORM_tw_count_excl_quest_marks',\n",
    "       'NORM_tw_count_special1', 'NORM_tw_count_hash',\n",
    "       'NORM_tw_last_quest', 'NORM_tw_len_retweet', 'NORM_tw_len_rt',\n",
    "       'NORM_tw_count_at', 'NORM_tw_count_words', 'NORM_tw_count_char',\n",
    "       'NORM_tw_rt_count_words', 'NORM_tw_rt_count_char',\n",
    "       'NORM_LOG_b_follower_count', 'NORM_LOG_b_following_count',\n",
    "       'NORM_b_follows_a']\n",
    "CAT_FEATS = [f for f in FEATURES if not 'NORM' in f]\n",
    "NORM_FEATS = [f for f in FEATURES if 'NORM' in f]\n",
    "IDX = np.load(f'../jun-5-2021-RECSYS/group{MODEL_NUM}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aded58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model()\n",
    "    \n",
    "model.load_weights('nn%i.h5'%85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53deedfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 19, 144, 177, 130,  26, 153, 195,  56, 207, 192, 137,  48, 173,\n",
       "        59, 122, 187,  85,  27,  73, 108,  81,  89, 151, 157,  58,  36,\n",
       "        10, 141, 119,   9,  13, 154,  71, 135, 194,  80,  21, 212,  50,\n",
       "        70, 203, 176, 156, 148, 186,   0,  47, 160, 181, 106, 172,  97,\n",
       "        11, 103,  88,  15, 149,  45, 158,  46,  52, 124,   8, 120, 175,\n",
       "       128,  42, 188,   5,  67, 216,   3, 121,  96, 113, 184, 211, 166,\n",
       "       115,  83, 155, 146, 109,   4, 214, 197,  93, 183,  72, 164, 178,\n",
       "        91,  34, 116, 170,  12,  51,  17, 101, 190,  60, 127,  39, 145,\n",
       "        53, 105,  22, 213,  62,  75, 165,  33, 138,  20,  84, 204, 210,\n",
       "       111, 107, 174,  69, 191, 100,  99, 112, 133, 206,  61,  44, 123,\n",
       "         6,  55,  86, 179, 152,  63, 202, 196, 147,  76, 117,  23,  78,\n",
       "         2, 205, 150,  57, 132,  54,  32, 143, 168, 215,  25, 198,  31,\n",
       "        30, 102,  74, 134,  14,  68,  64,  98,   1,  40, 129,  79, 199,\n",
       "        37,  43, 180, 104, 189, 169,  28, 136,  95, 209, 118,  18, 200,\n",
       "       185,  49, 167,  29,  87, 162,  35, 139,  16, 182, 131, 161, 140,\n",
       "        77, 142,  82, 110, 163,  24,  65,   7,  41,  66, 201, 159, 125,\n",
       "        90, 126,  94, 208, 217, 114, 193,  92, 171,  38])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts2 = np.arange(218)\n",
    "np.random.shuffle(parts2)\n",
    "print(len(parts2))\n",
    "parts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dc4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'part{MODEL_NUM}_c',parts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b4fa1",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c06332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### PART 0\n",
      "#########################\n",
      "(3256402, 37) , (2990814, 37) , (2992279, 37) , (2991089, 37) , (3242655, 37) , (2990724, 37) , (2991761, 37) , (3178602, 37) , (2874007, 37) , (2989905, 37) , (2991578, 37) , (3199173, 37) , (2991379, 37) , (3167030, 37) , (2991787, 37) , (45839185, 37) , (3256402, 48) , (2990814, 48) , (2992279, 48) , (2991089, 48) , (3242655, 48) , (2990724, 48) , (2991761, 48) , (3178602, 48) , (2874007, 48) , (2989905, 48) , (2991578, 48) , (3199173, 48) , (2991379, 48) , (3167030, 48) , (2991787, 48) , (45839185, 48) , LOG_a_follower_count , LOG_a_following_count , LOG_b_follower_count , LOG_b_following_count , NORM_LOG_a_follower_count , NORM_LOG_a_following_count , NORM_dt_minute , NORM_len_hashtags , NORM_len_links , NORM_len_domains , NORM_tw_len_media , NORM_tw_len_photo , NORM_tw_len_video , NORM_tw_len_gif , NORM_tw_len_quest , NORM_tw_len_token , NORM_tw_count_capital_words , NORM_tw_count_excl_quest_marks , NORM_tw_count_special1 , NORM_tw_count_hash , NORM_tw_last_quest , NORM_tw_len_retweet , NORM_tw_len_rt , NORM_tw_count_at , NORM_tw_count_words , NORM_tw_count_char , NORM_tw_rt_count_words , NORM_tw_rt_count_char , NORM_LOG_b_follower_count , NORM_LOG_b_following_count , NORM_b_follows_a , users 9411349\n",
      "[  505077737 -1322387926 -1727712552   352735204   717262158]\n",
      "WARNING:tensorflow:From /home/cdeotte/anaconda3/envs/rapids14/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 6 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 6 IndexedSlices\n",
      "4874/5596 [=========================>....] - ETA: 2:38 - loss: 0.2007"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3028/5544 [===============>..............] - ETA: 9:14 - loss: 0.1986"
     ]
    }
   ],
   "source": [
    "for PART_NUM in range(15):\n",
    "    print('#'*25)\n",
    "    print('### PART',PART_NUM)\n",
    "    print('#'*25)\n",
    "    \n",
    "    f = open(f'log_{VER}.txt','a')\n",
    "    f.write('#############################\\n')\n",
    "    f.write(f'### PART {PART_NUM}\\n')\n",
    "    #f.write('#############################\\n')\n",
    "    f.close()\n",
    "\n",
    "    # LOAD TRAIN FEATURES\n",
    "    data = []\n",
    "    parts = parts2\n",
    "    \n",
    "    CT = min(15, len(parts2) - PART_NUM*15)\n",
    "    \n",
    "    for x in range(CT):\n",
    "        name = 'part-%.5i.parquet'%parts[PART_NUM*15+x]\n",
    "        df = pd.read_parquet(f'/raid/RecSys/recsys2021/parquet7/'+name,columns=COLS)\n",
    "        data.append(df)\n",
    "        print( df.shape,', ',end='')\n",
    "    \n",
    "    train = pd.concat(data,axis=0,ignore_index=True)\n",
    "    del df, data\n",
    "    gc.collect()\n",
    "    print( train.shape ,', ',end='')\n",
    "\n",
    "    # LOAD TRAIN TOKENS\n",
    "    data = []\n",
    "    for x in range(CT):\n",
    "        name = 'part-%.5i.npy'%parts[PART_NUM*15+x]\n",
    "        df = np.load(f'/raid/RecSys/recsys2021/tweet_tokens/'+name)[:,:TOK_SIZE]\n",
    "        data.append(df)\n",
    "        print( df.shape ,', ',end='')\n",
    "    \n",
    "    train_tokens = np.concatenate(data,axis=0)\n",
    "    del df, data\n",
    "    gc.collect()\n",
    "    print( train_tokens.shape ,', ',end='')\n",
    "\n",
    "    # LOG PROCESS\n",
    "    TARGETS = ['reply','retweet','retweet_comment','like']\n",
    "    LOG_FEATS = ['a_follower_count','a_following_count','b_follower_count','b_following_count']\n",
    "    NUM_FEATS1 = []\n",
    "    for f in LOG_FEATS:\n",
    "        name = 'LOG_'+f\n",
    "        print(name,', ',end='')\n",
    "        train[name] = np.log1p( train[f].values, dtype='float32' )\n",
    "        #valid[name] = np.log1p( valid[f].values, dtype='float32' )\n",
    "        NUM_FEATS1.append( name )\n",
    "        del train[f]\n",
    "    gc.collect()\n",
    "\n",
    "    # NORM PROCESS\n",
    "    NORM_FEATS = ['LOG_a_follower_count','LOG_a_following_count',\n",
    "              'dt_minute','len_hashtags','len_links','len_domains']\n",
    "              #'a_account_creation','b_account_creation','a_is_verified','b_is_verified']\n",
    "\n",
    "    NORM_FEATS += ['tw_len_media', 'tw_len_photo','tw_len_video', 'tw_len_gif', \n",
    "          'tw_len_quest', 'tw_len_token',\n",
    "          'tw_count_capital_words', 'tw_count_excl_quest_marks',\n",
    "          'tw_count_special1', 'tw_count_hash', 'tw_last_quest', \n",
    "          'tw_len_retweet', 'tw_len_rt', 'tw_count_at', 'tw_count_words', 'tw_count_char',\n",
    "          'tw_rt_count_words', 'tw_rt_count_char']\n",
    "\n",
    "    NORM_FEATS += ['LOG_b_follower_count','LOG_b_following_count','b_follows_a']\n",
    "\n",
    "    data = pd.read_csv('../jun-5-2021-RECSYS/standardize_ALL.csv').set_index('feature')\n",
    "\n",
    "    NUM_FEATS2 = []\n",
    "    for f in NORM_FEATS:\n",
    "        name = 'NORM_'+f\n",
    "        print(name,', ',end='')\n",
    "        mn = data.loc[f,'mean']\n",
    "        st = data.loc[f,'std']\n",
    "        train[name] = ((train[f].values - mn) /st).astype('float32')\n",
    "        NUM_FEATS2.append( name )\n",
    "        del train[f]\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    if PART_NUM==0:\n",
    "        data = data.reset_index()\n",
    "        data.to_csv('standardize_%i.csv'%VER,index=False)\n",
    "        data.head()\n",
    "\n",
    "    # USER PROCESS\n",
    "    FILTER = 5\n",
    "    IDX = np.load(f'../jun-5-2021-RECSYS/group{MODEL_NUM}.npy')\n",
    "    print('users',len(IDX))\n",
    "    print( IDX[:5] )\n",
    "    user_map = {x:y for x,y in zip(IDX,1+np.arange(len(IDX)))}\n",
    "    if PART_NUM==0:\n",
    "        np.save('user_map_%i'%VER,IDX)\n",
    "    \n",
    "    train['a_user_id'] = train.a_user_id.map(user_map).fillna(0).astype('int32')\n",
    "    train['b_user_id'] = train.b_user_id.map(user_map).fillna(0).astype('int32')\n",
    "    CAT_FEATS = ['a_user_id','b_user_id','tweet_type','language','media','dt_dow']\n",
    "    #for f in CAT_FEATS:\n",
    "    #    m = train[f].max()\n",
    "    #    print(f,m)\n",
    "    \n",
    "    FEATURES = CAT_FEATS + NUM_FEATS2\n",
    "    #print( np.asarray( FEATURES ) )\n",
    "\n",
    "    # SHUFFLE TRAIN\n",
    "    #ID = np.arange(len(train))\n",
    "    #np.random.shuffle(ID)\n",
    "    #train = train.iloc[ID].reset_index(drop=True)\n",
    "    #train_tokens = train_tokens[ID,]\n",
    "\n",
    "    hh = model.fit([train[FEATURES],train_tokens],train[TARGETS],\n",
    "          #validation_data = ([valid[FEATURES],valid_tokens],valid[TARGETS]),\n",
    "          epochs=1, verbose=1, batch_size=1024*8)\n",
    "    \n",
    "    lss = hh.history['loss'][0]\n",
    "    f = open(f'log_{VER}.txt','a')\n",
    "    f.write(f'### LOSS {lss}\\n')\n",
    "    f.write('#############################\\n')\n",
    "    f.close()\n",
    "    \n",
    "    del train, train_tokens\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a312a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('nn%i.h5'%VER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
